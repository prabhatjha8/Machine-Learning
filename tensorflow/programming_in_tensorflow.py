# -*- coding: utf-8 -*-
"""Tensorflow+ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f-mqYAJz_1Cbj4UlRI0PsCkUl1L1mU2d
"""

import tensorflow as tf

import pandas as pd
import numpy as np
from tensorflow import keras

tf.__version__

a = tf.constant([[1,2,3], [4,5,6]])

print(a)

a+10

a*2

a**2

a + tf.constant([[1,2,3]])

a

a = tf.constant([[1,2,3], [4,5,6]])

tf.reshape(a, (3,2))

tf.zeros((3,4))

a > 3

a + np.array([[1], [2]])

np.mean(a)

np.mean(a, axis = 0)
a

print(np.max(a)) # This gives maximum across all the elements
print(np.max(a, axis=0)) # this gives maximum in the zeroth axis i.e along the one a[0, :]
print(a[0, :])
tf.reduce_mean(a, axis=[0])
# So basiaclly the max in the numpy is replaced by reduce_max in the tensorflow

print(tf.reduce_mean(tf.cast(a, dtype=tf.float32)))
print(tf.reduce_min(a, axis=1))
a

tf.constant(4)*tf.cast(tf.constant(2.0), dtype=tf.int32)
tf.constant(2)*tf.Variable(3)

tf.constant(4)

tv = tf.Variable([[1,2,3], [2,4,7]], trainable = False)
tv

tv[1,2].assign(100)

tv

tv + a

b = tv+a

b

b[1,1]
type(a)

tf.Variable(a)

a

tf.where([True, False, False, True],
         5,
         6).numpy()

tf.transpose(a)

import tensorflow as tf
from tensorflow import keras
import sklearn

a = tf.constant([1,2,3])
b = tf.constant([1,3,5])
tf.reduce_sum(a)

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()

print(type(housing))
housing["data"].shape

housing.keys()

X_data = housing["data"]
y_data = housing["target"]

X_train_full, X_test, y_train_full, y_test = train_test_split(X_data, y_data, test_size = 0.1)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size = 0.1)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_valid = scaler.transform(X_valid)

from sklearn.metrics import r2_score
model = keras.models.Sequential([keras.layers.Input(shape = [8]),
                                 keras.layers.Dense(10, activation = "selu", kernel_initializer = "lecun_normal"),
                                 keras.layers.Dense(12, kernel_initializer = "he_normal"),
                                 keras.layers.LeakyReLU(alpha = 0.2),
                                 keras.layers.Dense(1)
])

model.compile(loss = "mse", optimizer = "sgd")
model.fit(X_train, y_train, epochs=10, validation_data = (X_valid, y_valid))
model.evaluate(X_test, y_test)

model.layers[0].get_weights()[0].shape


# Different useful stuff we can get out of a layer are the following:

# Weights (weights): The trainable weights of the layer. You can access them using the get_weights() method on a layer instance. The weights are typically represented as numpy arrays.

# Trainable (trainable): A boolean parameter indicating whether the layer's weights are trainable or not. It can be set when creating a layer or modified using the trainable property.

# Number of Parameters (count_params()): The total number of trainable parameters in the layer. It can be calculated using the count_params() method on a layer instance.

# Input Shape (input_shape): The shape of the input data expected by the layer. It can be accessed using the input_shape property of the layer.

# Output Shape (output_shape): The shape of the output produced by the layer. It can be accessed using the output_shape property of the layer.

# Activation (activation): The activation function applied to the layer's output. It can be accessed using the activation property of the layer.

# Regularization (kernel_regularizer, bias_regularizer): Regularization functions applied to the layer's weights and biases, respectively. They can be set when creating a layer and provide regularization constraints during training.

# Constraints (kernel_constraint, bias_constraint): Constraint functions applied to the layer's weights and biases, respectively. They can be set when creating a layer and provide constraints on weight and bias values during training.

# Output (output): The output tensor of the layer. You can obtain the output by calling the layer on an input tensor, e.g., output = layer(input).

# Name (name): The name of the layer. It can be accessed using the name property of the layer.

model.layers[0].activation([-2.0])

model.evaluate(X_test, y_test)

model.predict(X_test[:10])

y_test[:10]

# Lets learn using and implementing custom loss function using tensorflow
# I want to check if this custom loss fucntion should be taking two
# values t_test value and y_pred value and return the loss or two y_test_list
# and y_pred list, so will build a fn to test. The function will be
# build in such a way that it can only input two numbers and not list

# def huber_loss1(true_value, predicted_value):
#   if abs(true_value - predicted_value)<1.0:
#     return 0.5*(true_value - predicted_value)**2
#   else:
#     return abs(true_value - predicted_value) - 0.5
# The above one is the wrong code

# conclusion: it takes two tf arrays and returns array of loss

def huber_fn(y_true, y_pred):
  error = y_true - y_pred
  is_small_error = tf.abs(error)<1
  squared_loss = tf.square(error)/2
  linear_loss = tf.abs(error)-0.5
  return tf.where(is_small_error, squared_loss, linear_loss)

# Now lets use this loss function to model california housing data

# huber_loss1(tf.constant([1, 2,3,4,5], dtype = "float32"), tf.constant([2,1,1,5,3], dtype = "float32"))

abs(tf.constant([1.0, 2.0,3.0,4.0,.5])- tf.constant([2.0,1.0,1.0,5.0,3.0]))**2

model = keras.models.Sequential([keras.layers.Input(shape = [8]),
                                 keras.layers.Dense(32, activation = "selu", kernel_initializer = "lecun_normal"),
                                 keras.layers.Dense(16, activation = "elu", kernel_initializer = "he_normal"),
                                 keras.layers.Dense(8, kernel_initializer = "he_normal"),
                                 keras.layers.LeakyReLU(alpha = 0.2),
                                 keras.layers.Dense(1)
])


model.compile(loss = huber_fn, optimizer = "sgd", metrics = [keras.metrics.mean_squared_error])
model.fit(X_train, y_train, epochs=30, validation_data = (X_valid, y_valid))

model.evaluate(X_test, y_test)
# The output is an array of two elements, the first one is the loss value for tbe test set while the second is the value of the metric for the test set

# model.predict(X_test[:10])
model.summary()

# y_test[:10]
input = keras.layers.Flatten(input_shape = [28,28])
print(input)
input_prime = keras.layers.Input(shape = [8])
print(input_prime)

model = keras.models.Sequential()
model.add(keras.layers.Dense(32, activation = "relu", kernel_initializer = "he_normal", kernel_regularizer = keras.regularizers.l2(0.02)))
model.add(keras.layers.Dense(20, activation = "selu", kernel_initializer = "lecun_normal"))
model.add(keras.layers.Dropout(0.5))
model.add(keras.layers.Dense(1))

model.compile(loss = "huber", optimizer = "sgd", metrics = ["mse"])
model.fit(X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid))

# So the learning is; while using sequential api, we dont need to specify the size of the input until
# its flat and we are using the dense layers, it good practice to flatten the layer if image data is
# used which is done by using the following: keras.layers.Flatten(input_shape = [28,28]).

# Using the Functional Api of keras, here we need to specify the input layer

input = keras.layers.Input(shape = [8])
hidden1 = keras.layers.Dense(32, activation = "selu", kernel_initializer = "lecun_normal")(input)
hidden2 = keras.layers.Dense(16, activation = "elu", kernel_initializer = "he_normal")(hidden1)
hidden3 = keras.layers.Dense(8, kernel_initializer = "he_normal")(hidden2)
leaky_relu_layer = keras.layers.LeakyReLU(alpha = 0.2)(hidden3)
concat = keras.layers.concatenate([leaky_relu_layer, input])
output = keras.layers.Dense(1)(concat)
model = keras.Model(inputs = [input], outputs = [output])
model.compile(loss = "huber_loss", optimizer = "sgd", metrics = [keras.metrics.mean_squared_error])
model.fit(X_train, y_train, epochs=30, validation_data = (X_valid, y_valid))

model.evaluate(X_test, y_test)

# So the above works i.e if we use loss function which takes input values rather than list
# of values it works, now lets use the huber loss function which just takes list as inputs

def huber_loss_list(test_list, predicted_list):
  return tf.math.minimum(0.5*tf.square(test_list-predicted_list), tf.math.abs(test_list-predicted_list)-0.5)

# def huber_loss_list():
  # return None

input = keras.layers.Input(shape = [8])
hidden1 = keras.layers.Dense(32, activation = "selu", kernel_initializer = "lecun_normal")(input)
hidden2 = keras.layers.Dense(16, activation = "elu", kernel_initializer = "he_normal")(hidden1)
hidden3 = keras.layers.Dense(8, kernel_initializer = "he_normal")(hidden2)
leaky_relu_layer = keras.layers.LeakyReLU(alpha = 0.2)(hidden3)
concat = keras.layers.concatenate([leaky_relu_layer, input])
output = keras.layers.Dense(1)(concat)
model.compile(loss = huber_loss_list, optimizer = "sgd", metrics = [keras.metrics.mean_squared_error])
model.fit(X_train, y_train, epochs=43, validation_data = (X_valid, y_valid))

model.evaluate(X_test, y_test)

model.predict(X_test[:10])

y_test[:10]

# learning: The constomized function taking values and not array wont work, also a function taking
# arrays doesn't easily wrok due to ambiguity around how exactly it's used inside, suggestion: write
# a general function using tensorflow which doesn't have assumptions around shape size etc of the input

# Save the model
model = model.save("first_save.h5")

model2 = keras.models.load_model("first_save.h5", custom_objects = {"huber_loss_list":huber_loss_list})

model2.evaluate(X_test, y_test)

# Learning: Basically saving the model is same as before you just use command model.save(name), loading requires an extra item custom_objects which is a dictionaty with name of the function in string as key and the actual function as value

# Now as we know that if we create the general huber loss function with the facility of threshold,
# it won't be saved while saving the model, it could be handled using subclassing the keras.losses.Loss
# class, follow the below to understand it

class HuberLoss(keras.losses.Loss):
  def __init__ (self, threshold = 1.0, **kwargs):
    self.threshold = threshold
    super().__init__(**kwargs)

  def call(self, y_true , y_pred):
    error = tf.abs(y_true - y_pred)
    is_error_small = error < self.threshold
    squared_loss = tf.square(y_true - y_pred)/2.0
    linear_loss = self.threshold*error - 0.5*(self.threshold**2)
    return tf.where(is_error_small, squared_loss, linear_loss)

  def get_config(self):
    base_config = super().get_config()
    return {**base_config, "threshold":self.threshold}

input = keras.layers.Input(shape = [8])
hidden1 = keras.layers.Dense(32, activation = "selu", kernel_initializer = "lecun_normal")(input)
hidden2 = keras.layers.Dense(16, activation = "elu", kernel_initializer = "he_normal")(hidden1)
hidden3 = keras.layers.Dense(8, kernel_initializer = "he_normal")(hidden2)
leaky_relu_layer = keras.layers.LeakyReLU(alpha = 0.2)(hidden3)
concat = keras.layers.concatenate([leaky_relu_layer, input])
output = keras.layers.Dense(1)(concat)
model = keras.Model(inputs = [input], outputs = [output])
model.compile(loss = HuberLoss(0.7), optimizer = "sgd", metrics = [keras.metrics.mean_squared_error])
model.fit(X_train, y_train, epochs=60, validation_data = (X_valid, y_valid))

# Saving the model

model.save("Custom_loss_ANN.h5")

# Loading the model

model3 = keras.models.load_model("Custom_loss_ANN.h5", custom_objects={"HuberLoss":HuberLoss})

model3.evaluate(X_test, y_test)

# Custom Activation function, initializers, regularizers and constraints

# We will implement custom softplus activation, galrot_normal initializer,
# l1 regularizer and non_neg_weight_constraint, which is available in keras
# as keras.activations.softplus(), keras.initializers.galrot_normal(),
# keras.regularizers.l1(0.01), keras.constraints.nonneg()

def my_softplus(z):
  return (tf.math.log(tf.math.exp(z)+1.0))

def my_galrot_initializers(shape, dtype = tf.float32):
  stddev = tf.sqrt(2.0/(shape[0]+shape[1]))
  return tf.random.normal(shape, mean = 0, stddev=stddev, dtype = dtype)

def my_l1_regularizer(weights):
  return tf.reduce_sum(tf.abs(0.01*weights))

def my_nonneg_constraints(weights):
  return tf.math.maximum(tf.zeros(weights.shape), weights)

input = keras.layers.Input(shape = [8])
hidden1 = keras.layers.Dense(32, activation = "selu", kernel_initializer = "lecun_normal", kernel_constraint = my_nonneg_constraints)(input)
hidden2 = keras.layers.Dense(16, activation = my_softplus, kernel_initializer = "he_normal", kernel_regularizer = my_l1_regularizer)(hidden1)
hidden3 = keras.layers.Dense(8, kernel_initializer = my_galrot_initializers)(hidden2)
leaky_relu_layer = keras.layers.LeakyReLU(alpha = 0.2)(hidden3)
concat = keras.layers.concatenate([leaky_relu_layer, input])
output = keras.layers.Dense(1)(concat)
model = keras.Model(inputs = [input], outputs = [output])
model.compile(loss = HuberLoss(0.7), optimizer = "sgd", metrics = [keras.metrics.mean_squared_error])
model.fit(X_train, y_train, epochs=10, validation_data = (X_valid, y_valid))

# Now lets create the regularizer using subclassing technique as we have a threshold there

class MyL1Regularizer(keras.regularizers.Regularizer):
  def __init__(self, alpha, **kwargs):
    self.alpha = alpha
    super().__init__(**kwargs)
  def call(self, weights):
    return tf.reduce_sum(tf.abs(self.alpha*weights))
  def get_config(self):
    base_config = super().get_config()
    return {**base_config, "alpha":self.alpha}

input = keras.layers.Input(shape = [8])
hidden1 = keras.layers.Dense(20, activation = "selu", kernel_initializer = "lecun_normal", kernel_constraint = my_nonneg_constraints)(input)
hidden2 = keras.layers.Dense(14, activation = my_softplus, kernel_initializer = "he_normal", kernel_regularizer = MyL1Regularizer(0.01))(hidden1)
hidden3 = keras.layers.Dense(8, kernel_initializer = my_galrot_initializers)(hidden2)
leaky_relu_layer = keras.layers.LeakyReLU(alpha = 0.2)(hidden3)
concat = keras.layers.concatenate([leaky_relu_layer, input])
output = keras.layers.Dense(1)(concat)
model = keras.Model(inputs = [input], outputs = [output])
model.compile(loss = HuberLoss(0.7), optimizer = "sgd", metrics = [keras.metrics.mean_squared_error])
model.fit(X_train, y_train, epochs=30, validation_data = (X_valid, y_valid))

# Now we shall learn how to use custom metrics
# To create the streaming metric (one which doesn't avg out results for each batch
# instead find the aggregated one on the entire dataset) the subclassing of keras.metrics.Metric can be done as follow

def create_huber(threshold):
  def huber_fn(y_true, y_pred):
    error = tf.abs(y_true - y_pred)
    is_error_small = error < threshold
    squared_loss = tf.square(y_true - y_pred)/2.0
    linear_loss = threshold*error - 0.5*(threshold**2)
    return tf.where(is_error_small, squared_loss, linear_loss)
  return huber_fn

# class HuberMetric(keras.metrics.Metric):
#   def __init__(self, threshold = 1.0, **kwargs):
#     super().__init__(**kwargs)
#     self.threshold = threshold
#     self.huber_metric = create_huber(self.threshold)
#     self.total = self.add_weight("total", initializer = "zeros")
#     self.count = self.add_weight("count", initializer = "zeros")

#   def update_state(self, y_true, y_pred, sample_weight = None):
#     metric = self.huber_metric(y_true, y_pred)
#     self.total.assign_add(tf.reduce_sum(metric))
#     print(self.total)
#     self.count.assign_add(tf.cast(tf.size(y_true), dtype = tf.float32))


#   def result(self):
#     return tf.reduce_sum(self.total)/self.count

#   def get_config(self):
#     base_config = super().get_config()
#     return {**base_config, "threshold":self.threshold}

class HuberMetric(keras.metrics.Metric):
  def __init__(self, threshold = 1.0, **kwargs):
    super().__init__(**kwargs)
    self.threshold = threshold
    self.huber_metric = create_huber(self.threshold)
    self.total = tf.Variable(0.0, dtype = tf.float32)
    self.count = tf.Variable(0.0, dtype = tf.float32)

  def update_state(self, y_true, y_pred, sample_weight = None):
    metric = self.huber_metric(y_true, y_pred)
    self.total.assign_add(tf.reduce_sum(metric))
    print(self.total)
    self.count.assign_add(tf.cast(tf.size(y_true), dtype = tf.float32))


  def result(self):
    return tf.reduce_sum(self.total)/self.count

  def get_config(self):
    base_config = super().get_config()
    return {**base_config, "threshold":self.threshold}

# a= tf.Variable(0.0, dtype = tf.float32)
# a.assign_add(tf.Variable(5, dtype = tf.float32))
# a

hm = HuberMetric(1.0)
hm.total
hm.update_state(tf.Variable([1.0,2.0,3.0,4.0]), tf.Variable([1.1, 2.4, 4.2, 3.9]))
print(hm.count)
hm.result()

input = keras.layers.Input(shape = [8])
hidden1 = keras.layers.Dense(20, activation = "selu", kernel_initializer = "lecun_normal", kernel_constraint = my_nonneg_constraints)(input)
hidden2 = keras.layers.Dense(15, activation = my_softplus, kernel_initializer = "he_normal", kernel_regularizer = MyL1Regularizer(0.01))(hidden1)
hidden3 = keras.layers.Dense(8, kernel_initializer = my_galrot_initializers)(hidden2)
leaky_relu_layer = keras.layers.LeakyReLU(alpha = 0.2)(hidden3)
concat = keras.layers.concatenate([leaky_relu_layer, input])
output = keras.layers.Dense(1)(concat)
model = keras.Model(inputs = [input], outputs = [output])
model.compile(loss = HuberLoss(0.7), optimizer = "sgd", metrics = [HuberMetric(1.0)])
model.fit(X_train, y_train, epochs=10, validation_data = (X_valid, y_valid))

# Building a new layer

# Two types of layers: one without weight (such as Flatten layer) other with weights
# One without weight can be built using the lambda class of keras.layers as follows:

exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))

# So to build a custom layer, we need to build the subclass of keras.layers.Layer class, following is an exam ple fo the custom layer

class MyDense(keras.layers.Layer):
  def __init__(self, units, activation = None, **kwargs):
    self.units = units
    self.activation = keras.activations.get(activation)
    super().__init__(**kwargs)

  def build(self, batch_input_shape):
    self.kernel = self.add_weight(name = "kernel", shape = (batch_input_shape[-1], self.units), initializer = 'glorot_normal')
    self.bias = self.add_weight(name = "bias", shape = self.units, initializer  = "zeros")
    super().build(batch_input_shape) # must be at the end

  def call(self, X):
    return self.activation(X @ self.kernel + self.bias)

  def get_config(self):
    base_config = super().get_config()
    return {**base_config, "units":self.units, "activation": keras.activations.serealize(self.acivation)}

input = keras.layers.Input(shape = [8])
hidden = MyDense(32, activation = "relu")(input)
output = keras.layers.Dense(1)(hidden)
model = keras.Model(inputs = [input], outputs = [output])

model.compile(loss = "huber_loss", optimizer = "sgd", metrics = [HuberMetric(0.7)])
model.fit( X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid))

# Creating layer with multiple inputs like "Concatenate", build the call() method with tuple or list of inputs and return the list of outputs for multiple outputs

class MyMultiLayer(keras.layers.Layer):
  def call(self, X):
    X1, X2 = X
    return ([X1+X2, X1*X2, X1/X2])

  def compute_output_shape(self, batch_input_shape):
    b1, b2 = batch_input_shape
    return [b1, b1, b1]

# Third type of layer is the one which has different type of behaviour during training and differet while testing, for example dropout, batch normalization
# Lets add a layer, which adds gaussian noise to the input while training and doesn't do anything during tetsing time or in production

class MyGaussianNoise(keras.layers.Layer):
  def __init__(self, stddev, **kwargs):
    self.stddev = stddev
    super().__init__(**kwargs)

  def call(self, X, training = None):
    if training:
      return X+tf.random.normal(tf.shape(X), stddev = self.stddev)
    else:
      return X

  def compute_output_shape(self, batch_input_shape):
    return batch_input_shape

# With this we have finished building three four types of layers: one without weights which is just a trasformation and could be done using lambda function,
# the second kind of layer are with weights as vanilla dense layer, one with multiple inputs and outputs like concat, one with different behaviour during training and testing times

a = tf.Variable([[1,2,3],[2,3,4]])
b = tf.Variable([[1],[2]])
a*b

from tensorflow.python.ops.init_ops_v2 import Initializer
class MyCustomModel(keras.Model):
  def __init__(self, n_units, n_layers, **kwargs):
    super().__init__(**kwargs)
    self.units = n_units
    self.n_layers = n_layers
    self.hidden = [keras.layers.Dense(self.units, activation = "selu", kernel_initializer = "lecun_normal") for i in range(n_layers)]
    self.main_output = keras.layers.Dense(1)

  def call(self, input):
    Z = self.hidden[0](input)
    for i in range(1, self.n_layers):
      Z = self.hidden[i](Z)
    main_output = self.main_output(Z)
    return main_output

model = MyCustomModel(30, 5)

model.compile(loss = "mse", optimizer = "sgd", metrics = ["mse"])
model.fit(X_train, y_train, epochs = 20, validation_data = [X_valid, y_valid])

"""
There are many ways of building ANN, using keras sequential api, keras functional api, subclassing api.
Keras sequential api can be used like keras.models.Sequential(), so basiaclly either you write model = keras.models.Sequential() and list of layers inside the bracket, or you keep adding layers using the command model.add(layer_is_added_here)
Now, there are different types of layer, keras.layers.Input(shape = [8]), dense layer: keras.layers.Dense(), keras.layers.Concatenate(list of layer names),
Now, what are the arguments for these layers, for dense layer, keras.layers.Dense(units = 32, activation = "relu", kernel_initializer = "glorot_normal", kernel_regularizer = keras.regularizers.l1(0.01), kernel_constraint = MaxNorm(3.0))
In sequential, we dont need to have the input layer, in case of images when we have to flatten item, we can use keras.layers.Flatten(input_shape = [28, 28]), in case the input is already flatten one can directly start with the dense layer
While using functional API, we need to define the input layer (input layer can not be flatten in case of imgaes, it has to be keras.layers.Input(shape = [inout_shape])) and the all layers one by one, here a layer for example keras.layers.Dense() is a function and takes as input the previous layer following is an example:
input = keras.layers.Input(shape = [28,28])
layer_1 = keras.layers.Dense(32, activation = "relu", kernel_initializer = "lecun_normal")(input)
and once this web of layer and its inputs are fed in, i.e achitecture is built, we define the model as follows:
model = keras.Model(inputs = [input], outputs = [output])
After the model is built, we compile the model and then fit the model
model.compile(loss = "mse", optimizer="sgd", metrics = [set of metric put it here])
model.fit(X_train, y_train, epochs = 30, validation_data =(X_valid, y_valid))
model.evaluate(X_test, y_test)
model.predict(X_new)

How to crete custom objects:
First recall what objects can be customized
1) loss (subclass of keras.losses.Loss, write init function, with threshold, **kwargs, use super().__init__(**kwargs), call() function takes input y_true and y_ored and resturn list of loss values for instances, the aggregation/averaging of this is done through the **kwargs and the get_config() function at the end)
2) activation 3) initialzer 4) regularizer 5) weight_constraint (same as loss, make init, call and get_config the input for activation would be a real number, for initializer it takes shape, regularizer and my_constraints takes weights)
6) Metric: This has to be streaming metric, init method, update_sate, result, get_config
7) Layer: init, build, call, get_config here in build add super().__init__(**kwargs) at the end, it takes shape as input and form the shape (shape[-1], units), call takes as input X and return the final output after going through the layer
8) Model: it must have call function, which decides the flow through different layers, layers and all should be build within init fn, also if training: xyz else: abc sort of logic could be used in call

"""

import time
a = time.time()
print(a)
a = time.time()
# time.sleep(2)
b = time.time()
print(b-a)
import tensorflow as tf
from tensorflow import keras
a = tf.constant([1.0,2.0,3.0])
b = tf.constant([1.0,2.0,5.0])
print(b-a)
tf.constant(len(tf.constant([0.0, 0.0, 2.0])), dtype = tf.float32)
# tf.minimum(b,a)

# Lets first revise to build a metric and layer and then custom model (time granted = 1 hr) post that we shall learn rest of the part of chap 12. In the evening we shall start chap 13. Sat and sun for chapter 14.

# I will create the Huber metric, which takes in the threshold and resurn the function which can take input y_true and y_pred and resturn the value of loss not the arrat like that in case of loss, in loss the aggregation etc to is taken care by the class constructor of keras.metrics.Metric

def create_huber_riv(threshold):
  def Huber_Fn_Riv(y_true, y_pred):
    is_error_small = abs(y_true-y_pred)<threshold #even abs of tensor is tensor outputting the absolute value, tf.abs could also be used
    squared_loss = 0.5*(y_true-y_pred)**2
    linear_loss = threshold*(abs(y_true-y_pred)) - 0.5*(threshold**2)
    return tf.where(is_error_small, squared_loss, linear_loss)
  return Huber_Fn_Riv

def create_huber_riv2(threshold):
  def Huber_Fn_Riv2(y_true, y_pred):
    return tf.minimum(0.5*(y_true-y_pred)**2, threshold*(abs(y_true-y_pred)) - 0.5*(threshold**2))
  return Huber_Fn_Riv2

# So basically the create metric is function is done, now i will use subclassing

class MyHuber(keras.metrics.Metric):
  def __init__(self, threshold = 1.0, **kwargs):
    super().__init__(**kwargs)
    self.threshold = threshold
    self.total = self.add_weight(name = "total", initializer = "zeros", dtype = tf.float32)
    self.count = self.add_weight(name = "count", initializer = "zeros", dtype = tf.float32)
    self.huber_val = create_huber_riv(threshold)


  def update_state(self, y_true, y_pred, sample_weight = None):
    self.total.assign_add(tf.reduce_mean(self.huber_val(y_true, y_pred)))
    self.count.assign_add(tf.cast(tf.size(y_true), dtype=tf.float32))

  def result(self):
    return self.total/self.count

  def get_config(self, **kwargs):
    base_config = super().get_config(**kwargs)
    return {**base_config, "threshold":self.threshold}


# input = keras.layers.Input(shape = [8])
# hidden = keras.layers.Dense(32, activation = "relu")(input)
# output = keras.layers.Dense(1)(hidden)
# model = keras.Model(inputs = [input], outputs = [output])

# model.compile(loss = "huber_loss", optimizer = "sgd", metrics = [MyHuber(0.7)])
# model.fit( X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid))



import tensorflow as tf
from tensorflow import keras
import numpy as np

# Now we will build custom layers
# There are four types of layers,
# 1) No weights, it just applies a function to inputs
# 2) One like dense layer, with weights, shape, initializer ...
# 3) One like concatenate
# 4) One which acts while training but not testing, like gaussian noise, batch normatization

# we will write all four

# Lets begin with the first

exponential_layer = keras.layers.Lambda(lambda z: tf.exp(z))

# Lets write dense layer, here we need to have, init, build function, call and get config

class MyDenseRev(keras.layers.Layer):
  def __init__(self, units, activation, **kwargs):
    super().__init__(**kwargs)
    self.units = units
    self.activation = keras.activations.get(activation)

  def build(self, input_shape):
    shap = (input_shape[-1], self.units)
    self.weight = self.add_weight(name="weight", shape = shap, initializer = keras.initializers.glorot_normal)
    self.bias = self.add_weight(name="bias", shape = self.units, initializer = "zeros")
    super().build(input_shape)

  def call(self, X):
    return self.activation( X@self.weight + self.bias)

  def get_config(self):
    base_config = super().get_config()
    return {**base_config, "units":self.units, "activation":keras.activations.serealize(self.activation)}


input = keras.layers.Input(shape = [8])
hidden = MyDenseRev(32, "relu")(input)
output = keras.layers.Dense(1)(hidden)
model = keras.Model(inputs = [input], outputs = [output])

model.compile(loss = "huber_loss", optimizer = "sgd", metrics = [MyHuber(0.7)])
model.fit( X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid))

# Lets build third type of layer

class my_conc_rev(keras.layers.Layer):
  def __init__(self, **kwargs):
    super().__init__(**kwargs)

  def call(self, X):
    X1, X2 = X
    return [X1+X2, X2*X2, X1/X2]

# Lets make fourth type of layer
# Lets build a layer, which takes the input a
class MyGaussianRev(keras.layers.Layer):
  def __init__(self, units, activation, **kwargs):
    super().__init__(**kwargs)
    self.units = units
    self.activation = keras.activations.get(activation)

  def build(self, input_shape):
    shap = (input_shape[-1], self.units)
    self.weight = self.add_weight(name = "weigh", shape = shap, initializer = keras.initializers.lecun_normal)
    self.bias = self.add_weight(name = "bias", shape = self.units, initializer = "zeros")

  def call(self, X, training = None):
    if training:
      X = tf.square(X) + tf.exp(tf.random.normal(shape = tf.shape(X), stddev = 1.0))
    else:
      X = tf.square(X)
    return self.activation(X@self.weight + self.bias)

  def get_config(self):
    base_config = super().get_config()
    return {**base_config, "units":self.units, "activation":keras.activations.get(self.activation)}



input = keras.layers.Input(shape = [8])
hidden = MyGaussianRev(32, activation = "selu")(input)
output = keras.layers.Dense(1)(hidden)
model = keras.Model(inputs = [input], outputs = [output])

model.compile(loss = "huber_loss", optimizer = "sgd", metrics = [MyHuber(0.7)])
model.fit( X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid))

# Lets Recall building custom mdoels

# Custom models: Two functions here under the subclass of keras.Model, first is init and second is call, call takes input the X and resturn the final output, now define model = Classname() and compile and if fit it, done

# First i will built a custom layer which takes applies two dense and the concat the input and the the output of the dense

class atrangi_layer(keras.layers.Layer):
  def __init__(self, n_layers, **kwargs):
    super().__init__(**kwargs)
    self.hidden = [keras.layers.Dense(8, activation = "selu", kernel_initializer = "lecun_normal") for i in range(n_layers)]

  def call(self, X):
    Y = X
    for i in self.hidden:
      X = i(X)
    return X+Y

class atrangi_model(keras.Model):
  def __init__(self, n_recurrent, **kwargs): # first layer would be simple dense, n atrangi layers with first one being three times recurrent, last is again dense
    super().__init__(**kwargs)
    self.n_recurrent = n_recurrent
    self.first_layer = keras.layers.Dense(8, activation = "selu", kernel_initializer = "lecun_normal")
    self.atrangi1 =  atrangi_layer(2)
    self.atrangi2 =  atrangi_layer(2)
    self.outp = keras.layers.Dense(1)

  def call(self,X):
    X = self.first_layer(X)
    for i in range(self.n_recurrent):
      X = self.atrangi1(X)
    X = self.atrangi2(X)
    return self.outp(X)

model = atrangi_model(2)

model.compile(loss = "mse", optimizer = "sgd", metrics = [MyHuber(0.7)])
model.fit( X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid))

# This works just that the the loss values etc overflows so the output are nans
# Learning: never define a layer in call method of the model class, it must be in the constructor itself

tf.constant(2)

# Will implement the residual block as done in the book

import tensorflow as tf
from tensorflow import keras
import numpy as np

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()
X_data = housing["data"]
y_data = housing["target"]
X_train_full, X_test, y_train_full, y_test = train_test_split(X_data, y_data, test_size = 0.1)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size = 0.1)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_valid = scaler.transform(X_valid)

# class ResidualBlock(keras.layers.Layer):
#   def __init__(self, n_layers, **kwargs):
#     super().__init__(**kwargs)
#     self.hidden = [keras.layers.Dense(8, activation = "elu", kernel_initializer = "he_normal") for _ in range(n_layers)]

#   def call(self, inputs):
#     Z = inputs
#     for layer in self.hidden:
#       Z = layer(Z)
#     return inputs + Z

# class ResidualRegressor(keras.Model):
#   def __init__(self, output_dim, **kwargs):
#     super().__init__(**kwargs)
#     self.hidden1 = keras.layers.Dense(8, activation = "elu", kernel_initializer = "he_normal")
#     self.block1 = ResidualBlock(2)
#     self.block2 = ResidualBlock(1)
#     self.out = keras.layers.Dense(output_dim)

#   def call(self, inputs):
#     Z = self.hidden1(inputs)
#     # Z = self.block1(Z)
#     for i in range(2):
#       Z = self.block1(Z)
#     Z = self.block2(Z)
#     return self.out(Z)


# model = ResidualRegressor(1)
# model.compile(loss = "mse", optimizer = "sgd")
# model.fit(X_train, y_train, epochs = 15, validation_data = (X_valid, y_valid))

#trying a tweak

class ResidualBlock(keras.layers.Layer):
  def __init__(self, n_layers, **kwargs):
    super().__init__(**kwargs)
    self.hidden = [keras.layers.Dense(20, activation = "elu", kernel_initializer = "he_normal") for _ in range(n_layers)]
    self.hidden_last = keras.layers.Dense(8, activation = "elu", kernel_initializer = "he_normal")

  def call(self, inputs):
    Z = inputs
    for layer in self.hidden:
      Z = layer(Z)
    return inputs + self.hidden_last(Z)

class ResidualRegressor(keras.Model):
  def __init__(self, output_dim, **kwargs):
    super().__init__(**kwargs)
    self.hidden1 = keras.layers.Dense(8, activation = "elu", kernel_initializer = "he_normal")
    self.block1 = ResidualBlock(2)
    self.block2 = ResidualBlock(1)
    self.out = keras.layers.Dense(output_dim)

  def call(self, inputs):
    Z = self.hidden1(inputs)
    # Z = self.block1(Z)
    for i in range(2):
      Z = self.block1(Z)
    Z = self.block2(Z)
    return self.out(Z)


model = ResidualRegressor(1)
model.compile(loss = "mse", optimizer = "sgd")
model.fit(X_train, y_train, epochs = 15, validation_data = (X_valid, y_valid))

# This works, nan's are due to computation overflow i guess

# Another type of custom model is where the loss is computed not only on the final outputs but also on the losses based on model internal configuration maybe, for that we use self.add_loss(value) in the call function and the value is compute dinside teh build method inside the class, lets see an exmaple
# Look at the exmaple

import tensorflow as tf
from tensorflow import keras
import numpy as np

class ReconstructingRegressor(keras.Model):
  def __init__(self, output_dim, **kwargs):
    super().__init__(**kwargs)
    self.output_dim = output_dim
    self.hidden = [keras.layers.Dense(30, activation="selu", kernel_initializer = "lecun_normal") for i in range(5)]
    self.out = keras.layers.Dense(output_dim)

  def build(self, batch_input_shape):
    n_inputs = batch_input_shape[-1]
    self.reconstruct = keras.layers.Dense(n_inputs)
    super().build(batch_input_shape)

  def call(self, inputs):
    Z = inputs
    for layers in self.hidden:
      Z = layers(Z)
    reconstruction = self.reconstruct(Z)
    recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
    self.add_loss(0.05*recon_loss)
    return self.out(Z)

model = ReconstructingRegressor(1)
model.compile(loss = "mse", optimizer = "sgd", metrics = ["mse"])
model.fit(X_train, y_train, epochs = 20, validation_data =(X_valid, y_valid))

# Now we learn, how to find gradient etc using tensorflow which would be used to train super custom models further.
# Lets learn autodiff

def f(w1, w2):
  return (3*w1**2 + 2*w1*w2)

w1, w2 = tf.Variable(5.0), tf.Variable(3.0)
with tf.GradientTape() as tape:
  z = f(w1, w2)

gradients = tape.gradient(z, [w1, w2])
gradients
# Once we start the tape, the tape stats recording all the operation, and tape.gradient(fn, list of variables) can only be run once not more than that, if there is a need of running it more than once use inside tf.GradientTape(persistent = True)

# Gradients can not be found wrt to constants, for exmaple
c1, c2 = tf.constant(3.0), tf.constant(2.0)
with tf.GradientTape() as tape:
  z = f(c1, c1)

tape.gradient(z, [c1,c2])

# But we can force the tensorflow to watch the constants too and force it to find out the derivative wrt it too

d1, d2 = tf.constant(5.0), tf.constant(3.0)

with tf.GradientTape(persistent = True) as tape:
  tape.watch(d1)
  tape.watch(d2)
  z = f(d1, d2)

print(tape.gradient(z, d1))
print(tape.gradient(z, d2))

def my_softplus(z):
  return tf.math.log(tf.exp(z)+1)

z = tf.Variable(100.0)

with tf.GradientTape(persistent = True) as tape:
  f = my_softplus(z)

tape.gradient(f, z)

# This does not output a real value due to neumerical difficulties, it could be handled to an extent by
# analytically finding it derivative and customly letting it know to tensorflow as follows:
# Discussed in the next block

@tf.custom_gradient
def my_better_softplus(z):
  ex = tf.exp(z)
  def softplus_gradient(w):
    return 1/(1 + tf.exp(-w))
  return tf.math.log(tf.exp(z)+1), softplus_gradient

# Now when we compute the gradient of "my_better_softplus" function, we get the porper result, even for larger input values (however the main output still explodes because of the exponential; on eworkaround is use tf.where()to return the input when they are large), let try it

z = tf.Variable(100.0)

with tf.GradientTape(persistent = True) as tape:
  f = my_better_softplus(z)

tape.gradient(f, z)

# Now as the above worked now we shall learn coding the custom training loop
import tensorflow as tf
from tensorflow import keras
import numpy as np

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()
X_data = housing["data"]
y_data = housing["target"]
X_train_full, X_test, y_train_full, y_test = train_test_split(X_data, y_data, test_size = 0.1)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size = 0.1)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_valid = scaler.transform(X_valid)

l2_reg = keras.regularizers.l2(0.05)
model = keras.models.Sequential([keras.layers.Dense(30, activation ="elu", kernel_initializer = "he_normal", kernel_regularizer = l2_reg),
                                 keras.layers.Dense(1, kernel_regularizer = l2_reg)])

# The nmodel need not be compiled as we will customly do that

# Now we define random batch in the below code
def random_batch(X, Y, batch_size=32):
  idx = np.random.randint(len(X), size = batch_size)
  return X[idx], Y[idx]

# Now we will define a function that will display the training loop

def print_status_bar(iteration, total, loss, metrics = None):
  metrics = "-".join(["{}:{:.4f}".format(m.name, m.result()) for m in [loss]+(metrics or [])])
  end = "" if iteration < total else "\n"
  print("\r{}/{} - ".format(iteration, total) + metrics, end = end)

# Now we will do the main thing setting up few items and defining the training loop

n_epochs = 5
batch_size = 32
n_steps = len(X_train)//batch_size
optimizer = keras.optimizers.Nadam(lr=0.01)
loss_fn = keras.losses.mean_squared_error # Computes the mean squared error between labels and predictions.
mean_loss = keras.metrics.Mean() # This is a subclass of keras.metrics.Metric, so it has fns update_state(y_true, y_pred) and result()
metrics = [keras.metrics.MeanAbsoluteError()]

# Now lets build the custom loop

for epochs in range(1, n_epochs+1):
  print(f"Epoch: {epochs}/{n_epochs}")
  for step in range(1, n_steps+1):
    X_batch, y_batch = random_batch(X_train, y_train, batch_size)
    with tf.GradientTape() as tape:
      y_pred = model(X_batch, training = True)
      main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
      loss = tf.add_n([main_loss]+model.losses) #model.losses here computes other losses such as regularization losses for two layers
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    # mean_loss(loss)
    # for metric in metrics:
    #   metric(y_batch, y_pred)
    #   print(print_status_bar(step*batch_size, len(y_train), mean_loss, metric))
    # print(print_status_bar(len(y_train), len(y_train), mean_loss, metric))
    for metric in [mean_loss]+metrics:
      metric.reset_states()

X_train[[1,2,3]]
metric = [1,2,3]
(metric or [])
keras.losses.mean_squared_error([1.0,2.0], [1.7, 1.9])
print(model.predict(X_test[:10]))
print(y_test[:10])

model.losses

l = [1,2,4, 1, 3]
print(l.sort())

def solution(A):
    A = sorted(A)
    n = len(A)
    for i in range(n):
        if A[i]>0:
            break
    if i==n-1 or A[i] != 1:
        return 1
    else:
        while i<=n-2 and A[i+1]-A[i]<=1:
            i+=1
    if i == n-2 and A[-1]-A[-2]<=1:
      return A[-1]+1
    elif i == n-2 and A[-1]-A[-2]>1:
      return A[-2]+1
    else:
      return A[i]+1

print(solution([-1, 2]))

def solution(A):
  A = sorted(A)
  n = len(A)
  def bin_search(l, a):
    start = 0
    end = len(1)-1
    mid = start+end//2
    if l[mid] < a:
      return bin_search(l[:mid], a)
    elif:
      return bin_search(l[mid+1], a)
    else:
      return mid

def bin_search(l, a):
    if len(l) <=1:
      return 0
    start = 0
    end = len(l)-1
    mid = (start+end)//2
    print("mid", mid)
    if l[mid] < a:
      return mid + 1 + bin_search(l[mid+1:], a)
    elif l[mid] > a:
      return bin_search(l[:mid], a)
    else:
      return mid

l = [1,2,4,17, 26, 40]
l[:3]
bin_search(l, 4.5)

