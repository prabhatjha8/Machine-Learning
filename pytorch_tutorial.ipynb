{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ PyTorch Fundamentals: A Comprehensive Guide",
    "",
    "Welcome to this comprehensive PyTorch tutorial! This notebook is designed to be your one-stop resource for understanding the core concepts of PyTorch, from basic tensor manipulations to 
building and training neural networks. We'll explore key functionalities, best practices, and \"mantras\" to remember along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch",
    "import torch",
    "import numpy as np",
    "import torch.nn as nn",
    "import sklearn",
    "from sklearn import datasets",
    "from sklearn.preprocessing import StandardScaler",
    "from sklearn.model_selection import train_test_split",
    "from sklearn.metrics import accuracy_score",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Core PyTorch Concepts",
    "",
    "Let's start by understanding the fundamental building blocks of PyTorch: **Tensors**.",
    "",
    "### What is a Tensor?",
    "In PyTorch, **tensors** are the fundamental data structure, similar to NumPy arrays but with the added capability of running on GPUs and enabling automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª Device Management: CPU vs. GPU",
    "",
    "One of PyTorch's most powerful features is its ability to leverage Graphics Processing Units (GPUs) for accelerated computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üî¢ Tensor Operations",
    "",
    "PyTorch tensors support a wide array of operations, many of which are similar to NumPy.",
    "",
    "### Tensor Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors",
    "tensor_2d = torch.tensor([[1, 2, 3], [5, 6, 7]])",
    "print(f\"2D Tensor:\\n{tensor_2d}\")",
    "",
    "# Specifying data type",
    "float_tensor = torch.tensor([[1, 2, 3], [5, 6, 7]], dtype=torch.float)",
    "print(f\"Float Tensor:\\n{float_tensor}\")",
    "",
    "int_tensor = torch.tensor([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]], dtype=torch.int32)",
    "print(f\"Int32 Tensor:\\n{int_tensor}\")",
    "",
    "# Creating empty tensors (uninitialized data)",
    "empty_tensor_1d = torch.empty(3)",
    "empty_tensor_2d = torch.empty((3, 5)) # Can also be torch.empty(3, 5)",
    "print(f\"Empty 1D Tensor (values are random):\\n{empty_tensor_1d}\")",
    "",
    "# Creating tensors with specific initial values",
    "zeros_tensor = torch.zeros(4, 6, dtype=torch.int32)",
    "ones_tensor = torch.ones((2, 3))",
    "print(f\"Zeros Tensor:\\n{zeros_tensor}\")",
    "print(f\"Ones Tensor:\\n{ones_tensor}\")",
    "",
    "# Creating tensors from existing data",
    "list_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.int32)",
    "print(f\"Tensor from list:\\n{list_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tensor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform distribution (0, 1)",
    "random_uniform = torch.rand(2, 3)",
    "print(f\"Uniform Random Tensor:\\n{random_uniform}\")",
    "",
    "# Normal distribution (mean 0, variance 1)",
    "random_normal = torch.randn(2, 3)",
    "print(f\"Normal Random Tensor:\\n{random_normal}\")",
    "",
    "# Generating random integer with specified range [low, high)",
    "random_integers = torch.randint(0, 10, (2, 3)) # Generates integers between 0 (inclusive) and 10 (exclusive)",
    "print(f\"Random Integer Tensor:\\n{random_integers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Arithmetic Operations",
    "",
    "PyTorch supports standard arithmetic operations, both as operators and as methods. In-place operations are denoted by a trailing underscore `_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 2)",
    "b = torch.rand(2, 2)",
    "",
    "print(f\"Tensor a:\\n{a}\")",
    "print(f\"Tensor b:\\n{b}\")",
    "",
    "# Addition",
    "print(f\"\\nAddition (a + b):\\n{a + b}\")",
    "print(f\"Addition (a.add(b)):\\n{a.add(b)}\")",
    "a_copy_for_inplace = a.clone() # Clone to show inplace effect without altering 'a' for other ops",
    "a_copy_for_inplace.add_(b)",
    "print(f\"Addition (a.add_(b) - inplace):\\n{a_copy_for_inplace}\")",
    "# Mantra: Operations with a trailing underscore `_` modify the tensor in-place.",
    "",
    "# Subtraction",
    "print(f\"\\nSubtraction (a - b):\\n{a - b}\")",
    "a_copy_for_inplace = a.clone()",
    "a_copy_for_inplace.sub_(b)",
    "print(f\"Subtraction (a.sub_(b) - inplace):\\n{a_copy_for_inplace}\")",
    "",
    "# Element-wise Multiplication",
    "print(f\"\\nMultiplication (a * b):\\n{a * b}\")",
    "print(f\"Multiplication (a.mul(b)):\\n{a.mul(b)}\")",
    "a_copy_for_inplace = a.clone()",
    "a_copy_for_inplace.mul_(b)",
    "print(f\"Multiplication (a.mul_(b) - inplace):\\n{a_copy_for_inplace}\")",
    "",
    "# Element-wise Division",
    "print(f\"\\nDivision (a / b):\\n{a / b}\")",
    "print(f\"Division (a.div(b)):\\n{a.div(b)}\")",
    "a_copy_for_inplace = a.clone()",
    "a_copy_for_inplace.div_(b)",
    "print(f\"Division (a.div_(b) - inplace):\\n{a_copy_for_inplace}\") # recall : a will be replaced by a/b",
    "",
    "# Matrix Multiplication",
    "tensor1 = torch.tensor([[1, 2, 3], [5, 6, 7]], dtype=torch.float)",
    "tensor2 = torch.tensor([[1, 2],[3, 4], [5, 6]], dtype=torch.float)",
    "print(f\"\\nMatrix Multiplication (tensor1.matmul(tensor2)):\\n{tensor1.matmul(tensor2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of all elements",
    "print(f\"\\nSum of all elements in tensor1: {torch.sum(tensor1)}\")",
    "",
    "# Sum along a specific dimension (axis)",
    "print(f\"Sum along axis 0 (columns): {torch.sum(tensor1, axis=0)}\") # Sums elements column-wise",
    "print(f\"Sum along axis 1 (rows): {torch.sum(tensor1, axis=1)}\")    # Sums elements row-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing, Slicing, and Broadcasting",
    "",
    "These operations behave very similarly to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing",
    "d = torch.rand(5, 3)",
    "print(f\"\\nOriginal tensor d:\\n{d}\")",
    "print(f\"Element at [1, 2]: {d[1, 2]}\") # Accesses the element at row 1, column 2 (0-indexed)",
    "print(f\"Column 1 (all rows): {d[:, 1]}\")",
    "print(f\"Shape of column 1: {d[:, 1].shape}\") # Returns a 1D tensor",
    "print(f\"Row 1 (all columns): {d[1, :]}\")",
    "print(f\"Shape of row 1: {d[1, :].shape}\") # Returns a 1D tensor",
    "",
    "# Direct assignment (unlike some TensorFlow versions, direct assignment works)",
    "print(f\"\\nTensor 'a' before assignment:\\n{a}\")",
    "a[1, 1] = 100.0",
    "print(f\"Tensor 'a' after assignment (a[1, 1] = 100.0):\\n{a}\")",
    "",
    "# Broadcasting",
    "# When performing operations between tensors of different shapes, PyTorch (like NumPy)",
    "# can \"broadcast\" the smaller tensor across the larger one.",
    "tensor_broadcast_example = torch.tensor([[1, 2, 3], [2, 3, 4]])",
    "scalar_to_add = torch.tensor([1, 2, 3])",
    "print(f\"\\nTensor for broadcasting:\\n{tensor_broadcast_example}\")",
    "print(f\"Scalar to add: {scalar_to_add}\")",
    "print(f\"Result of broadcasting (addition):\\n{tensor_broadcast_example + scalar_to_add}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pya = torch.rand(4, 6)",
    "print(f\"\\nOriginal tensor pya:\\n{pya}\")",
    "print(f\"Shape of pya: {pya.shape}\")",
    "print(f\"Size of pya: {pya.size()}\")",
    "# Mantra: .shape and .size() both return a torch.Size object representing the tensor's dimensions.",
    "",
    "# Reshaping using .view() - returns a new tensor with the same underlying data but a different shape.",
    "# The total number of elements must remain the same.",
    "# -1 infers the dimension",
    "pyb_flattened = pya.view(24) # Flatten to 1D",
    "pyb_inferred_rows = pya.view(8, -1) # 8 rows, columns inferred",
    "pyc_inferred_cols = pya.view(-1, 2) # 2 columns, rows inferred",
    "pyd_specific_shape = pya.view(3, 8)",
    "",
    "print(f\"\\nFlattened (pyb_flattened):\\n{pyb_flattened}\")",
    "print(f\"Reshaped (8 rows, inferred cols - pyb_inferred_rows):\\n{pyb_inferred_rows}\")",
    "print(f\"Reshaped (inferred rows, 2 cols - pyc_inferred_cols):\\n{pyc_inferred_cols}\")",
    "print(f\"Reshaped (3 rows, 8 cols - pyd_specific_shape):\\n{pyd_specific_shape}\")",
    "",
    "# Use .item() to extract a single value from a 0-dimensional tensor",
    "single_value_tensor = torch.tensor(123.45)",
    "print(f\"\\nSingle value tensor: {single_value_tensor}\")",
    "print(f\"Extracted value: {single_value_tensor.item()}\")",
    "# Mantra: Use .item() only for tensors with a single element. Trying to use it on multi-element tensors will raise an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üîÑ NumPy and PyTorch Interoperability",
    "",
    "Seamless conversion between NumPy arrays and PyTorch tensors is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy to PyTorch",
    "numpy_array = np.ones((5, 2))",
    "torch_tensor_from_numpy = torch.from_numpy(numpy_array)",
    "",
    "print(f\"\\nNumPy array:\\n{numpy_array}\")",
    "print(f\"PyTorch tensor from NumPy:\\n{torch_tensor_from_numpy}\")",
    "",
    "# PyTorch to NumPy",
    "torch_tensor_to_numpy = torch.ones(5)",
    "numpy_array_from_torch = torch_tensor_to_numpy.numpy()",
    "",
    "print(f\"\\nPyTorch tensor to NumPy:\\n{torch_tensor_to_numpy}\")",
    "print(f\"NumPy array from PyTorch:\\n{numpy_array_from_torch}\")",
    "",
    "# Mantra: When a CPU PyTorch tensor is converted to a NumPy array, they share the same memory location.",
    "# Changes to one will reflect in the other.",
    "print(\"\\n--- Memory Sharing (CPU) ---\")",
    "a_cpu = torch.ones(5)",
    "b_numpy_from_cpu = a_cpu.numpy()",
    "a_cpu.add_(1) # In-place addition",
    "print(f\"PyTorch tensor after add_ (CPU): {a_cpu}\")",
    "print(f\"NumPy array after PyTorch change (CPU): {b_numpy_from_cpu}\")",
    "",
    "# Mantra: GPU tensors CANNOT be directly converted to NumPy arrays.",
    "# You must first move the tensor to the CPU using `.cpu()`.",
    "print(\"\\n--- GPU to NumPy Conversion ---\")",
    "pkt = torch.tensor([1.2, 2.3, 3.4], device=device)",
    "print(f\"GPU Tensor: {pkt}, on device: {pkt.is_cuda}\")",
    "# This would cause an error: pkt.numpy()",
    "pkt_cpu_numpy = pkt.cpu().numpy()",
    "print(f\"NumPy array from GPU tensor (after .cpu()): {pkt_cpu_numpy}\")",
    "# Mantra: For a GPU tensor to be converted to NumPy, it must first be moved to CPU (`.cpu()`).",
    "# If it also requires gradients, it must be detached (`.detach()`) first.",
    "# Combined: `tensor.detach().cpu().numpy()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üìâ Automatic Differentiation: `autograd`",
    "",
    "PyTorch's `autograd` engine is what makes it powerful for neural networks. It automatically computes gradients for all operations on tensors that have `requires_grad=True`.",
    "",
    "### `requires_grad=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)",
    "print(f\"\\nTensor x with requires_grad=True:\\n{x}\")",
    "",
    "y = x + 2",
    "print(f\"Tensor y (y = x+2):\\n{y}\")",
    "# Notice the 'grad_fn' attribute, indicating that PyTorch is tracking operations for gradient computation.",
    "",
    "z = 2 * (y**2)",
    "print(f\"Tensor z (z = 2 * (y**2)):\\n{z}\")",
    "",
    "w = torch.mean(z) # w is a scalar, which is typical for loss functions",
    "print(f\"Tensor w (w = mean(z)):\\n{w}\")",
    "",
    "# Perform backpropagation",
    "w.backward() # Computes gradients of w with respect to all tensors that have requires_grad=True and are part of the computation graph.",
    "print(f\"Gradient of w wrt x (x.grad):\\n{x.grad}\")",
    "# Mantra: `backward()` is called on a scalar output. If the output is a non-scalar (vector/matrix),",
    "# you must provide a `gradient` argument (often a vector of ones) to `backward()`. This is essentially",
    "# computing a Jacobian-vector product.",
    "",
    "# Example with vector output (Jacobian-vector product)",
    "x_vec = torch.randn(3, requires_grad=True)",
    "z_vec = torch.square(x_vec) # Output is a vector",
    "print(f\"\\nx_vec: {x_vec}\")",
    "print(f\"z_vec (x_vec**2): {z_vec}\")",
    "",
    "# If we want dz/dx for each component, we can pass a vector of ones to backward",
    "# This effectively sums the gradients of each component of z_vec with respect to x_vec.",
    "# For example, if z_vec = [f1(x), f2(x), f3(x)], then z_vec.backward(torch.ones_like(z_vec))",
    "# computes sum(df_i/dx).",
    "z_vec.backward(torch.tensor([1.0, 1.0, 1.0]))",
    "print(f\"Gradient of z_vec wrt x_vec (x_vec.grad) with vector input to backward:\\n{x_vec.grad}\")",
    "",
    "# If you only want the gradient of a specific component",
    "x_vec_single = torch.randn(3, requires_grad=True)",
    "z_vec_single = torch.square(x_vec_single)",
    "print(f\"\\nx_vec_single: {x_vec_single}\")",
    "# To get dz_vec_single[0]/dx_vec_single, pass [1, 0, 0]",
    "z_vec_single.backward(torch.tensor([1.0, 0.0, 0.0]))",
    "print(f\"Gradient of z_vec_single[0] wrt x_vec_single: {x_vec_single.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Accumulation",
    "",
    "By default, gradients accumulate. This means if you call `backward()` multiple times on different parts of the graph, the gradients will be added up in `x.grad`. This is useful for accumulating 
gradients over mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4, dtype=torch.float32, requires_grad=True)",
    "",
    "print(\"\\n--- Gradient Accumulation ---\")",
    "for epoch in range(3):",
    "    model_output = (3 * weights).sum()",
    "    model_output.backward()",
    "    print(f\"Epoch {epoch} - Accumulated weights.grad: {weights.grad}\")",
    "",
    "# Mantra: Gradients accumulate by default. Always zero them out before a new backpropagation pass.",
    "print(\"\\n--- Gradient Zeroing ---\")",
    "weights_reset = torch.ones(4, dtype=torch.float32, requires_grad=True)",
    "for epoch in range(3):",
    "    model_output_reset = (3 * weights_reset).sum()",
    "    model_output_reset.backward()",
    "    print(f\"Epoch {epoch} - weights_reset.grad before zero_(): {weights_reset.grad}\")",
    "    weights_reset.grad.zero_() # Reset gradients to zero",
    "    print(f\"Epoch {epoch} - weights_reset.grad after zero_(): {weights_reset.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preventing Gradient Computation",
    "",
    "There are several ways to stop PyTorch from tracking gradients for a tensor:",
    "",
    "1.  **`tensor.requires_grad_(False)`**: Changes the `requires_grad` attribute in-place.",
    "2.  **`tensor.detach()`**: Returns a new tensor that shares the same data but `requires_grad=False`.",
    "3.  **`with torch.no_grad():`**: A context manager that disables gradient calculation for all operations within its block. This is commonly used during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)",
    "print(f\"\\nOriginal x: {x}\")",
    "",
    "# Method 1: requires_grad_(False)",
    "x_nograd_inplace = x.clone()",
    "x_nograd_inplace.requires_grad_(False)",
    "print(f\"x_nograd_inplace (after requires_grad_(False)): {x_nograd_inplace}\")",
    "",
    "# Method 2: .detach()",
    "x_detached = x.detach()",
    "print(f\"x_detached (after .detach()): {x_detached}\")",
    "",
    "# Method 3: with torch.no_grad()",
    "with torch.no_grad():",
    "    y_no_grad = x + 2",
    "print(f\"y_no_grad (within torch.no_grad()): {y_no_grad}\")",
    "# Attempting y_no_grad.backward() here would raise an error because no graph was built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üõ†Ô∏è Building Models with PyTorch",
    "",
    "Now, let's put it all together to build and train machine learning models. We'll start with linear regression from scratch and then leverage PyTorch's `nn` module for a more streamlined 
approach.",
    "",
    "### Manual Linear Regression (Numpy Analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Your original NumPy LinearRegression and LogisticRegression classes are excellent for understanding",
    "# the manual calculations. I'm omitting them here for brevity in the final PyTorch-focused tutorial,",
    "# but they are valuable for foundational understanding.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with PyTorch (`autograd` only)",
    "",
    "Here, we'll use PyTorch tensors and `autograd` to calculate gradients, but still manually manage the weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)",
    "",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)",
    "",
    "def forward_manual(x):",
    "    \"\"\"Manual forward pass: y_pred = w * x\"\"\"",
    "    return w * x",
    "",
    "def loss_manual(y_true, y_predicted):",
    "    \"\"\"Manual MSE loss calculation\"\"\"",
    "    return ((y_true - y_predicted)**2).mean()",
    "",
    "learning_rate = 0.01",
    "n_iters = 100",
    "",
    "print(\"\\n--- Manual Linear Regression with PyTorch autograd ---\")",
    "for epoch in range(n_iters):",
    "    # Forward pass: compute prediction and loss",
    "    y_pred = forward_manual(X)",
    "    l = loss_manual(y, y_pred)",
    "",
    "    # Backward pass: compute gradient of loss with respect to weights",
    "    l.backward()",
    "",
    "    # Update weights (manual step, outside graph)",
    "    with torch.no_grad(): # Essential to prevent this operation from being part of the graph",
    "        w.data -= learning_rate * w.grad # Use .data to access underlying tensor without affecting grad tracking",
    "",
    "    # Zero gradients for the next iteration",
    "    w.grad.zero_()",
    "",
    "    if (epoch + 1) % 10 == 0:",
    "        print(f\"Epoch {epoch+1}/{n_iters}, Loss: {l.item():.4f}, Weight: {w.item():.4f}\")",
    "",
    "print(f\"\\nFinal predicted weight: {w.item():.4f}\")",
    "print(f\"Prediction for x=5: {forward_manual(torch.tensor(5.0)).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with PyTorch (`nn.Module`, Loss, and Optimizer)",
    "",
    "This is the standard PyTorch approach, leveraging built-in modules for model definition, loss calculation, and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Design Model: Input size, output size, forward pass",
    "# For nn.Linear, input should be (n_samples, n_features)",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)",
    "y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)",
    "",
    "n_samples, n_features = X.shape",
    "input_size = n_features",
    "output_size = y.shape[1] # For a single output, this is 1",
    "",
    "# nn.Linear(input_dim, output_dim) creates a linear layer (y = xW^T + b)",
    "model = nn.Linear(input_size, output_size)",
    "",
    "# 2) Construct Loss and Optimizer",
    "learning_rate = 0.01",
    "n_iters = 1000",
    "",
    "loss_fn = nn.MSELoss() # Mean Squared Error Loss",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Stochastic Gradient Descent",
    "",
    "print(\"\\n--- Linear Regression with PyTorch (nn.Module) ---\")",
    "for epoch in range(n_iters):",
    "    # Forward pass: compute prediction",
    "    y_pred = model(X)",
    "",
    "    # Calculate loss",
    "    l = loss_fn(y_pred, y)",
    "",
    "    # Backward pass: compute gradients",
    "    l.backward()",
    "",
    "    # Update weights (optimizer takes care of with torch.no_grad() internally)",
    "    optimizer.step()",
    "",
    "    # Zero gradients",
    "    optimizer.zero_grad()",
    "",
    "    if (epoch + 1) % 100 == 0:",
    "        [w, b] = model.parameters() # Unpack weights and bias",
    "        print(f\"Epoch {epoch+1}/{n_iters}, Loss: {l.item():.4f}, Weight: {w.item():.4f}, Bias: {b.item():.4f}\")",
    "",
    "print(f\"\\nPrediction for x=5: {model(torch.tensor([[5.0]]))[0].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Models with `nn.Module` (Subclassing)",
    "",
    "For more complex models, you'll often define your neural network by subclassing `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegCustom(nn.Module):",
    "    def __init__(self, input_size, output_size):",
    "        super().__init__() # Calls the constructor of the parent class (nn.Module)",
    "        # Define layers",
    "        self.linear_layer = nn.Linear(input_size, output_size)",
    "",
    "    def forward(self, x):",
    "        # Implement the forward pass (computations from input to output)",
    "        return self.linear_layer(x)",
    "",
    "# Instantiate the custom model",
    "model_custom = LinearRegCustom(input_size, output_size)",
    "",
    "# Training loop is identical to the previous example",
    "learning_rate = 0.01",
    "n_iters = 1000",
    "",
    "loss_fn_custom = nn.MSELoss()",
    "optimizer_custom = torch.optim.SGD(model_custom.parameters(), lr=learning_rate)",
    "",
    "print(\"\\n--- Custom Linear Regression (nn.Module Subclassing) ---\")",
    "for epoch in range(n_iters):",
    "    y_pred = model_custom(X)",
    "    l = loss_fn_custom(y_pred, y)",
    "    l.backward()",
    "    optimizer_custom.step()",
    "    optimizer_custom.zero_grad()",
    "",
    "    if (epoch + 1) % 100 == 0:",
    "        [w, b] = model_custom.parameters()",
    "        print(f\"Epoch {epoch+1}/{n_iters}, Loss: {l.item():.4f}, Bias: {b.item():.4f}\")",
    "",
    "print(f\"\\nPrediction for x=5 with custom model: {model_custom(torch.tensor([[5.0]]))[0].item():.4f}\")",
    "",
    "# Mantra: When you define a class that inherits from nn.Module, you only need to implement __init__ and forward.",
    "# The __call__ method (which is invoked when you do `model(X)`) is automatically handled by nn.Module,",
    "# which ensures that the forward method is called and hooks are registered for autograd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üöÄ Building a Logistic Regression Model (Binary Classification)",
    "",
    "Now let's apply these concepts to a binary classification problem using a logistic regression model. We'll utilize the Breast Cancer dataset.",
    "",
    "### Steps for Model Training in PyTorch",
    "",
    "1.  **Prepare Dataset**: Load, preprocess (scaling), and convert data to PyTorch tensors.",
    "2.  **Define Model**: Create the neural network architecture (e.g., using `nn.Module` subclassing).",
    "3.  **Define Loss Function and Optimizer**: Choose appropriate loss and optimization algorithms.",
    "4.  **Training Loop**: Iterate through epochs, perform forward pass, calculate loss, backward pass (gradient computation), and update weights.",
    "5.  **Evaluation**: Make predictions and assess model performance.",
    "",
    "### CPU Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Dataset preparation",
    "bc = datasets.load_breast_cancer()",
    "X_data, y_data = bc[\"data\"], bc[\"target\"]",
    "",
    "y_data = y_data.reshape((-1, 1))",
    "",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X_data, y_data, test_size=0.1, random_state=42)",
    "",
    "# Scaling features",
    "sc = StandardScaler()",
    "X_train_scaled = sc.fit_transform(X_train_np)",
    "X_test_scaled = sc.transform(X_test_np)",
    "",
    "# Convert to PyTorch tensors (on CPU first)",
    "X_train_tensor = torch.from_numpy(X_train_scaled.astype(np.float32))",
    "X_test_tensor = torch.from_numpy(X_test_scaled.astype(np.float32))",
    "y_train_tensor = torch.from_numpy(y_train_np.astype(np.float32))",
    "y_test_tensor = torch.from_numpy(y_test_np.astype(np.float32))",
    "",
    "# 1) Model definition",
    "n_samples, n_features = X_train_tensor.shape",
    "",
    "class LogisticRegressionModel(nn.Module):",
    "    def __init__(self, input_shape):",
    "        super().__init__()",
    "        self.linear_layer = nn.Linear(input_shape, 1) # Output size is 1 for binary classification",
    "",
    "    def forward(self, x):",
    "        return torch.sigmoid(self.linear_layer(x)) # Apply sigmoid for probabilities",
    "",
    "model_logreg_cpu = LogisticRegressionModel(n_features)",
    "",
    "# 2) Loss and Optimizer",
    "learning_rate_logreg = 0.01",
    "n_iters_logreg = 1000 # Increased iterations for better convergence",
    "",
    "# Mantra: For binary classification, use `nn.BCELoss()` with `torch.sigmoid` as the activation in the last layer.",
    "# If you don't apply sigmoid in the forward pass, you can use `nn.BCEWithLogitsLoss()`, which is numerically more stable.",
    "loss_fn_logreg = nn.BCELoss()",
    "optimizer_logreg = torch.optim.SGD(model_logreg_cpu.parameters(), lr=learning_rate_logreg)",
    "",
    "# 3) Training Loop",
    "print(\"\\n--- Logistic Regression (CPU) ---\")",
    "start_time_cpu = time.time()",
    "for epoch in range(n_iters_logreg):",
    "    # Forward pass (prediction + loss)",
    "    y_pred = model_logreg_cpu(X_train_tensor)",
    "    l = loss_fn_logreg(y_pred, y_train_tensor)",
    "",
    "    # Backward pass (compute gradients)",
    "    l.backward()",
    "",
    "    # Update weights + zero gradients",
    "    optimizer_logreg.step()",
    "    optimizer_logreg.zero_grad()",
    "",
    "    if (epoch + 1) % 100 == 0:",
    "        print(f\"Epoch {epoch+1}/{n_iters_logreg}, Loss: {l.item():.4f}\")",
    "",
    "# 4) Evaluation",
    "model_logreg_cpu.eval() # Set model to evaluation mode (disables dropout, batchnorm updates etc.)",
    "with torch.no_grad(): # No need to compute gradients during inference",
    "    test_pred_raw = model_logreg_cpu(X_test_tensor)",
    "    # Convert probabilities to binary predictions (0 or 1)",
    "    y_predictions_cpu = test_pred_raw.round().numpy() # No .detach() needed due to no_grad()",
    "    accuracy_cpu = accuracy_score(y_predictions_cpu, y_test_np)",
    "",
    "print(f\"\\nAccuracy on test set (CPU): {accuracy_cpu:.4f}\")",
    "end_time_cpu = time.time()",
    "print(f\"Training time (CPU): {end_time_cpu - start_time_cpu:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Implementation",
    "",
    "To leverage the GPU, we simply move our tensors and model to the `device` we defined earlier (`\"cuda\"` if available, else `\"cpu\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move tensors to the chosen device",
    "X_train_tensor_gpu = torch.from_numpy(X_train_scaled.astype(np.float32)).to(device)",
    "X_test_tensor_gpu = torch.from_numpy(X_test_scaled.astype(np.float32)).to(device)",
    "y_train_tensor_gpu = torch.from_numpy(y_train_np.astype(np.float32)).to(device)",
    "y_test_tensor_gpu = torch.from_numpy(y_test_np.astype(np.float32)).to(device)",
    "",
    "# Move the model to the chosen device",
    "model_logreg_gpu = LogisticRegressionModel(n_features).to(device)",
    "",
    "# Loss and Optimizer (same as CPU, but operations will happen on GPU)",
    "loss_fn_logreg_gpu = nn.BCELoss()",
    "optimizer_logreg_gpu = torch.optim.SGD(model_logreg_gpu.parameters(), lr=learning_rate_logreg)",
    "",
    "# Training Loop",
    "print(f\"\\n--- Logistic Regression ({device}) ---\")",
    "start_time_gpu = time.time()",
    "for epoch in range(n_iters_logreg):",
    "    # Forward pass (prediction + loss)",
    "    y_pred_gpu = model_logreg_gpu(X_train_tensor_gpu)",
    "    l_gpu = loss_fn_logreg_gpu(y_pred_gpu, y_train_tensor_gpu)",
    "",
    "    # Backward pass (compute gradients)",
    "    l_gpu.backward()",
    "",
    "    # Update weights + zero gradients",
    "    optimizer_logreg_gpu.step()",
    "    optimizer_logreg_gpu.zero_grad()",
    "",
    "    if (epoch + 1) % 100 == 0:",
    "        print(f\"Epoch {epoch+1}/{n_iters_logreg}, Loss: {l_gpu.item():.4f}\")",
    "",
    "# Evaluation",
    "model_logreg_gpu.eval()",
    "with torch.no_grad():",
    "    test_pred_raw_gpu = model_logreg_gpu(X_test_tensor_gpu)",
    "    # Mantra: To convert a GPU tensor to NumPy, it MUST be moved to CPU first (`.cpu()`).",
    "    y_predictions_gpu = test_pred_raw_gpu.round().cpu().numpy()",
    "    accuracy_gpu = accuracy_score(y_predictions_gpu, y_test_tensor_gpu.cpu().numpy()) # y_test also needs to be on CPU for numpy comparison",
    "",
    "print(f\"\\nAccuracy on test set ({device}): {accuracy_gpu:.4f}\")",
    "end_time_gpu = time.time()",
    "print(f\"Training time ({device}): {end_time_gpu - start_time_gpu:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üìö PyTorch Command Dictionary & Key Components",
    "",
    "Here's a quick reference for common PyTorch modules and functions.",
    "",
    "### `torch.nn` (Neural Network Layers and Modules)",
    "",
    "* **`nn.Module`**: Base class for all neural network modules. All your models should subclass this.",
    "* **`nn.Linear(input_features, output_features)`**: Applies a linear transformation ($y = xA^T + b$).",
    "* **Activation Functions (usually inside `nn.Module` or `torch.nn.functional`):**",
    "    * `nn.ReLU()`: Rectified Linear Unit.",
    "    * `nn.Sigmoid()`: Sigmoid activation (outputs probabilities between 0 and 1).",
    "    * `nn.Tanh()`: Hyperbolic Tangent activation.",
    "    * `nn.Softmax(dim)`: Softmax activation (outputs probability distribution over classes, sum to 1).",
    "    * `nn.LeakyReLU()`: Leaky ReLU.",
    "",
    "### `torch.nn.functional` (Functional API for Operations)",
    "",
    "* **`F.relu(input)`**: Functional version of ReLU. Often used directly in `forward` methods.",
    "* **`F.leaky_relu(input)`**: Functional version of Leaky ReLU.",
    "",
    "### Loss Functions (`nn.Module` subclasses)",
    "",
    "* **`nn.MSELoss()`**: Mean Squared Error (for regression).",
    "* **`nn.BCELoss()`**: Binary Cross-Entropy Loss (for binary classification with sigmoid output).",
    "* **`nn.BCEWithLogitsLoss()`**: Binary Cross-Entropy Loss with Sigmoid (more numerically stable, use directly on raw model output before sigmoid).",
    "* **`nn.CrossEntropyLoss()`**: Cross-Entropy Loss (for multi-class classification, combines `nn.LogSoftmax` and `nn.NLLLoss`). **Mantra**: Don't apply `torch.softmax` to your model's output 
before passing it to `nn.CrossEntropyLoss()`; the loss function expects raw logits.",
    "",
    "### Optimizers (`torch.optim`)",
    "",
    "* **`torch.optim.SGD(model.parameters(), lr=0.01)`**: Stochastic Gradient Descent.",
    "* **`torch.optim.Adam(model.parameters(), lr=0.01)`**: Adam optimizer (often performs better than SGD in many scenarios).",
    "* **`optimizer.step()`**: Performs a single optimization step (updates model parameters based on gradients).",
    "* **`optimizer.zero_grad()`**: Zeros the gradients of all optimized `torch.Tensor`s.",
    "",
    "### Example of a Simple Neural Network (Binary Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBinaryClassifier(nn.Module):",
    "    def __init__(self, input_size, hidden_size):",
    "        super().__init__()",
    "        self.linear1 = nn.Linear(input_size, hidden_size)",
    "        self.relu = nn.ReLU() # Activation layer",
    "        self.linear2 = nn.Linear(hidden_size, 1) # Output layer for binary classification",
    "",
    "    def forward(self, x):",
    "        x = self.linear1(x)",
    "        x = self.relu(x) # Apply ReLU activation",
    "        x = self.linear2(x)",
    "        return torch.sigmoid(x) # Output probabilities using sigmoid",
    "",
    "# Example usage:",
    "# model = MyBinaryClassifier(input_size=30, hidden_size=64)",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "## üí° Mantras to Remember",
    "",
    "* **`device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`**: Always define your device and move tensors/models to it for GPU acceleration.",
    "* **`tensor.add_(other_tensor)` (trailing underscore `_`)**: Operations with a trailing underscore `_` are **in-place** and modify the tensor directly.",
    "* **Gradients Accumulate**: Call `optimizer.zero_grad()` or `tensor.grad.zero_()` before each `backward()` call to prevent gradients from accumulating from previous iterations.",
    "* **`backward()` on Scalar**: `loss.backward()` works directly when the loss is a scalar. For vector-valued functions, pass a `gradient` argument to `backward()`.",
    "* **GPU to NumPy**: To convert a GPU tensor to a NumPy array, you must first move it to the CPU using `.cpu()`. If `requires_grad` is `True`, you also need `.detach()` first: 
`tensor.detach().cpu().numpy()`.",
    "* **`with torch.no_grad():`**: Use this context manager during inference (`model.eval()`) or any operation where you don't need to compute gradients, saving memory and computation.",
    "* **`nn.Module` Subclassing**: Implement `__init__` (to define layers) and `forward` (to define the computation graph).",
    "* **`nn.CrossEntropyLoss` vs. `nn.BCELoss`**:",
    "    * `nn.BCELoss()`: Use for binary classification when your model's final layer explicitly applies `torch.sigmoid`.",
    "    * `nn.BCEWithLogitsLoss()`: Use for binary classification when your model's final layer outputs raw logits (no sigmoid applied), as this function combines sigmoid and BCE for numerical 
stability.",
    "    * `nn.CrossEntropyLoss()`: Use for multi-class classification directly on raw logits (don't apply `torch.softmax` before passing to this loss).",
    "* **`model.train()` and `model.eval()`**: Switch between training and evaluation modes to properly handle layers like `Dropout` and `BatchNorm`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
