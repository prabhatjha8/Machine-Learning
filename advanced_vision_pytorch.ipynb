{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdGUXFpTXdKa",
        "outputId": "118e1639-5aa5-468b-e5fe-a3172df7c302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "import transformers"
      ],
      "metadata": {
        "id": "L6_PlrX_hXtX"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Learning Layers in PyTorch"
      ],
      "metadata": {
        "id": "sBXAAQ9uhY1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some gyan about fully connected layer\n",
        "\n",
        "# This is a fully connected layer or dense layer which has two main arguments in_features and out_features.\n",
        "# The in_feature argumnet specifies that it expects input of share (n1, n2,  . . . , n_k) where n_k is\n",
        "# equal to in_features. As the dense layer applies on the last dimension of the input, and after applying\n",
        "# the shape of the ouput would be (n1, n2,  . . . , out_features)\n",
        "\n",
        "fc_layer = nn.Linear(in_features = 100, out_features = 10)\n",
        "\n",
        "# If we want to apply activation function with this layer, we can wrap it into nn.sigmoid or nn.relu etc. Like nn.relu(fc_layer(input))\n",
        "\n",
        "# If we want to find the parameter of the layer, one way is fc_layer.parameters() its a generator object\n",
        "# and contains two items weights and bias, this methods paramters() is valid for models too, like if i do\n",
        "# model.parameter() that would also make a generator object of all parameter in the model.\n",
        "# But for fully connected layer like above, we can use fn_layer.weight and fn_laer.bias to get the weight\n",
        "# and bias of the dense layer\n",
        "\n",
        "# try\n",
        "for param in fc_layer.parameters():\n",
        "  print(param)\n",
        "  print(\"next param now\")\n",
        "\n",
        "print(\"weight of fc_layer: \", fc_layer.weight)\n",
        "print(\"bias of fc_layer: \", fc_layer.bias)\n",
        "\n",
        "# if we want to change intializer the weights in a specific way like he_initializer, lecun etc,\n",
        "# which was an argument with tf.Dense in tensorflow. It can be done as follows:\n",
        "\n",
        "\n",
        "# this is he_initialization, once this is run the weights are itself updated, nonlinearity arguments\n",
        "# sggests that this layer will be wrapped with relu activation thus so that the initialization should\n",
        "# be adjusted with that\n",
        "\n",
        "fc1 = nn.Linear(5, 2)\n",
        "nn.init.kaiming_normal_(fc1.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "# this is xavier initilization\n",
        "nn.init.xavier_uniform_(fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "# also, there isn't a constraint on choosing only the well known initializers, you can also\n",
        "# define the intilizer in a simple way with the initializer of your choice, for example\n",
        "\n",
        "fc1 = nn.Linear(5, 2)\n",
        "fc1.weight = nn.Parameter(torch.randn((5, 2)))\n",
        "fc1.bias = nn.Parameter(torch.zeros((2)))\n",
        "\n",
        "# Note: the fc1.weight update works without nn.Parameter wrapper but fc1.bias update does not work wihthout it.\n"
      ],
      "metadata": {
        "id": "AW_F63pQGgjo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# brief on conv2D layer\n",
        "\n",
        "# This is a 2D convolution layer which is commonly used in convolutional neural networks (CNNs)\n",
        "# for image processing tasks. It has several key arguments, the most important being in_channels,\n",
        "# out_channels, kernel_size, stride, and padding. in_channels specifies the number of input channels\n",
        "# (depth) that the layer expects, like 3 for RGB images. out_channels refers to the number of filters\n",
        "#  (or kernels) applied to the input. Each filter generates one output channel.\n",
        "# kernel_size specifies the size of the convolutional kernel (filter), typically as a\n",
        "# tuple (height, width) or a single integer if both dimensions are equal.\n",
        "# stride controls the step size of the convolution as it moves over the input.\n",
        "# padding adds zero-padding around the input to control the spatial size of the output.\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(4, 4), stride=1, padding=1)\n",
        "\n",
        "# This layer applies the convolution operation on the input tensor, and if your input has a shape of\n",
        "# (batch_size, 3, height, width), the output after this layer will be (batch_size, 16, height, width),\n",
        "# as we have set out_channels to 16 and padding=1 to maintain the spatial dimensions.\n",
        "# If we want to apply an activation function like ReLU or Sigmoid after the convolution, we can chain it like this:\n",
        "conv_output = nn.ReLU()(conv_layer(torch.randn(10, 3, 20, 20)))\n",
        "\n",
        "# To inspect the parameters of the convolutional layer (like weights and biases), we can use conv_layer.parameters(),\n",
        "# which, like the Linear layer, returns a generator object with two elements: the weights (filters) and biases for each filter.\n",
        "# for param in conv_layer.parameters():\n",
        "      # print(param)\n",
        "      # print(\"next param now\")\n",
        "\n",
        "# Specifically, you can access the convolution layer's filters (weights) and bias directly as follows:\n",
        "print(\"Weight of conv_layer (filters): \", conv_layer.weight)\n",
        "# the dimesnion of conv layer.weight is (out_channels, in_channels, filter_height, filter_eidth)\n",
        "\n",
        "print(\"Bias of conv_layer: \", conv_layer.bias)\n",
        "# it's a single dimensional vector with size equal to number of out_channels, the the above example its torch.Size([16])\n",
        "\n",
        "# Like fully connected layers, you can customize the initialization of the convolution filters. By default, PyTorch\n",
        "# uses Kaiming initialization for convolutional layers, but you can change it to another initializer if needed.\n",
        "# For example, you can apply He initialization (he is implemented in two ways uding the kaiming\n",
        "# , using normal or using uniform distribution ) (which is good for layers with ReLU activations):\n",
        "# nn.init.kaiming_normal_(conv_layer.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "# Or Xavier initialization:\n",
        "# nn.init.xavier_uniform_(conv_layer.weight, gain=nn.init.calculate_gain('relu'))\n",
        "# Note: gain=nn.init.calculate_gain('relu') is just to adjust the weights for relu activation\n",
        "\n",
        "# You can also define your custom initialization logic:\n",
        "# conv_layer.weight = nn.Parameter(torch.randn_like(conv_layer.weight)) conv_layer.bias = nn.Parameter(torch.zeros_like(conv_layer.bias))\n",
        "\n",
        "# Note: Similar to the fully connected layer, while the weights can be updated directly without wrapping in nn.Parameter, the biases require it to work correctly.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1WEb0qUUxUw",
        "outputId": "c641eef2-6cde-4b9c-e4ef-932a1114ea2b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight of conv_layer (filters):  Parameter containing:\n",
            "tensor([[[[-5.7463e-03,  1.2241e-01,  4.4320e-02,  1.3226e-01],\n",
            "          [-3.6728e-02, -3.1053e-02,  8.4526e-02,  1.8769e-02],\n",
            "          [ 1.3516e-02,  7.2797e-03, -1.1403e-01,  1.9078e-02],\n",
            "          [-9.8505e-02, -3.8978e-02, -6.7833e-04,  1.8371e-02]],\n",
            "\n",
            "         [[ 3.6775e-02, -4.3062e-02,  1.2054e-01, -6.6434e-02],\n",
            "          [-3.3405e-02, -1.1723e-01,  1.1306e-01,  1.0177e-01],\n",
            "          [ 1.1857e-01,  1.1485e-01,  1.0110e-02, -4.9143e-03],\n",
            "          [-1.0549e-01,  2.7721e-02,  5.2446e-02, -2.4568e-02]],\n",
            "\n",
            "         [[-1.8040e-03, -1.3481e-01, -4.5619e-02,  1.5098e-02],\n",
            "          [ 4.3307e-02,  8.1590e-02,  8.9634e-02,  9.7663e-02],\n",
            "          [ 2.8717e-02, -6.8920e-02, -1.0548e-02, -9.0130e-02],\n",
            "          [-1.0639e-02, -2.8493e-02,  5.1148e-02, -4.3359e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.4359e-02,  1.4711e-02,  1.3091e-02, -1.1513e-01],\n",
            "          [-1.0262e-01, -7.1928e-02, -3.7231e-02, -1.0603e-04],\n",
            "          [ 1.2334e-01, -7.9252e-02,  7.2890e-02,  1.1816e-01],\n",
            "          [-2.1331e-02,  7.3246e-02, -7.1484e-02, -3.7988e-02]],\n",
            "\n",
            "         [[ 6.1516e-02,  5.1050e-03, -1.4894e-03, -1.3092e-01],\n",
            "          [-3.2022e-02,  1.3976e-01,  1.4272e-01, -1.3300e-01],\n",
            "          [ 4.7673e-03,  8.7445e-02, -1.0745e-01, -5.6964e-03],\n",
            "          [ 3.2818e-02, -3.6534e-02,  4.3988e-03,  9.1278e-02]],\n",
            "\n",
            "         [[-4.6975e-02,  1.2082e-01, -4.8874e-02,  3.4042e-02],\n",
            "          [-3.9221e-03, -2.7621e-02,  3.6530e-02,  1.2602e-02],\n",
            "          [-5.4112e-02, -1.0973e-01,  1.1559e-01, -2.8106e-02],\n",
            "          [ 1.2264e-01,  8.6406e-02, -8.1390e-02, -9.6654e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.7998e-02,  4.1095e-02,  2.5377e-02,  1.2057e-01],\n",
            "          [ 6.9417e-02, -3.9382e-02,  7.1856e-02,  1.3834e-01],\n",
            "          [ 4.7334e-02,  1.4201e-01,  9.8379e-03,  1.2152e-01],\n",
            "          [-5.7398e-02, -6.3062e-03, -1.0631e-01,  1.3919e-01]],\n",
            "\n",
            "         [[ 5.4944e-02,  1.3860e-01, -1.2452e-01, -1.1165e-01],\n",
            "          [ 9.9625e-02, -5.5291e-03,  7.0395e-02,  1.2030e-01],\n",
            "          [-1.0891e-01, -2.3917e-02,  4.2239e-02,  2.2766e-02],\n",
            "          [-1.3230e-01,  2.4749e-02,  1.0932e-01,  5.3667e-02]],\n",
            "\n",
            "         [[-9.2203e-02,  1.3712e-01,  6.9802e-02, -9.1940e-02],\n",
            "          [-6.6509e-02,  6.6769e-02,  1.5625e-02,  1.1834e-01],\n",
            "          [ 1.3919e-01,  2.9336e-02,  3.1369e-03, -4.1307e-02],\n",
            "          [-3.3126e-02, -1.3700e-01, -5.4587e-03,  2.6745e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0874e-01, -2.7519e-02, -9.5398e-02,  3.9545e-02],\n",
            "          [ 1.1765e-01,  9.6268e-02, -6.3240e-02, -5.2581e-02],\n",
            "          [-4.5932e-02,  2.9539e-02,  7.4187e-03,  1.2866e-02],\n",
            "          [ 3.8359e-02, -1.1384e-01,  1.2833e-01, -1.1233e-01]],\n",
            "\n",
            "         [[-1.2134e-01,  2.9863e-02, -7.5835e-02,  6.9508e-02],\n",
            "          [ 3.1048e-02, -2.0366e-02, -3.8281e-02, -4.7182e-02],\n",
            "          [ 5.8868e-03,  3.1710e-02, -6.2518e-02, -8.1978e-02],\n",
            "          [-1.2888e-01,  2.6699e-02, -4.9909e-02, -4.9146e-02]],\n",
            "\n",
            "         [[ 4.8780e-02,  3.9397e-02, -1.2028e-01, -1.2529e-01],\n",
            "          [-9.0261e-02, -9.8538e-02,  1.0078e-01,  6.5371e-02],\n",
            "          [-6.8370e-02,  5.3083e-02,  6.9119e-02,  8.9726e-02],\n",
            "          [-4.4216e-02, -6.8217e-02,  1.1695e-01,  5.3246e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1167e-01,  7.2964e-04, -8.1306e-03,  2.5722e-03],\n",
            "          [ 1.3539e-01,  7.3686e-02, -7.8363e-02, -1.1551e-01],\n",
            "          [-9.1430e-02,  4.1647e-02, -6.1656e-02, -6.7663e-02],\n",
            "          [ 5.4583e-02, -1.2265e-01, -7.3192e-02,  5.8775e-02]],\n",
            "\n",
            "         [[ 1.0750e-01,  9.1060e-02,  9.3212e-02,  3.1960e-02],\n",
            "          [ 5.1042e-02,  5.9947e-02, -1.3019e-01,  1.2929e-02],\n",
            "          [-1.2837e-01, -2.1684e-02, -1.1654e-01, -1.3281e-01],\n",
            "          [-1.3829e-01, -4.6444e-02,  6.0392e-02, -1.3573e-01]],\n",
            "\n",
            "         [[-4.7700e-02, -4.3319e-02, -5.7557e-03, -9.1993e-02],\n",
            "          [-6.9712e-02, -4.8765e-03, -1.2501e-01,  3.5259e-02],\n",
            "          [ 1.0217e-01, -2.2989e-02,  1.2514e-02,  4.9395e-03],\n",
            "          [ 6.1892e-02,  2.1502e-03, -3.9457e-02,  6.6829e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3312e-01,  8.3545e-02, -9.3815e-02, -3.3862e-02],\n",
            "          [-1.1723e-01,  5.6322e-02,  6.6844e-02,  5.5109e-02],\n",
            "          [-3.3775e-02,  9.5710e-03, -1.0109e-01, -3.9361e-02],\n",
            "          [-1.2198e-01, -1.2406e-01,  8.2622e-02,  1.1637e-01]],\n",
            "\n",
            "         [[-2.2429e-02,  6.2162e-02, -1.1544e-01, -4.4636e-02],\n",
            "          [ 7.8315e-02,  8.6625e-02,  5.3278e-02,  4.9941e-02],\n",
            "          [ 1.1041e-01,  3.8560e-02,  6.3987e-02, -8.3946e-02],\n",
            "          [ 5.2715e-02, -8.8864e-03, -1.0671e-01, -5.8131e-02]],\n",
            "\n",
            "         [[ 1.2051e-01,  9.3747e-02, -2.8766e-02, -5.2128e-02],\n",
            "          [-1.2293e-01,  5.5524e-02, -2.4531e-02,  8.6609e-02],\n",
            "          [-1.1228e-01,  1.3197e-01,  1.3046e-01,  1.1133e-01],\n",
            "          [ 7.3452e-02,  8.8268e-02,  4.4206e-02, -2.8164e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2206e-01, -8.8527e-02, -8.8590e-02,  6.3051e-02],\n",
            "          [ 7.7883e-02,  1.2397e-01, -1.1866e-01,  5.0305e-02],\n",
            "          [ 1.1309e-01,  7.7575e-02,  3.7494e-02,  5.3335e-02],\n",
            "          [ 4.0563e-02, -4.9998e-04, -9.7368e-02, -3.9080e-02]],\n",
            "\n",
            "         [[-1.5005e-02, -1.2394e-01,  2.7423e-02,  1.7196e-02],\n",
            "          [ 7.6677e-02, -5.5045e-03,  9.4504e-02,  3.3066e-02],\n",
            "          [-4.1377e-02,  8.6840e-02, -6.7106e-02,  3.7111e-02],\n",
            "          [-4.1288e-02,  2.7720e-02,  4.4816e-02, -6.3656e-02]],\n",
            "\n",
            "         [[-7.7326e-02,  9.5576e-02,  4.1996e-02,  1.0789e-02],\n",
            "          [-9.5549e-02, -1.2201e-01, -1.1176e-01, -1.2601e-02],\n",
            "          [-6.3121e-02, -1.3621e-02, -7.6408e-02, -8.1660e-02],\n",
            "          [ 1.4824e-02,  7.7449e-03, -3.1901e-02, -6.6786e-02]]],\n",
            "\n",
            "\n",
            "        [[[-8.8551e-02, -7.5539e-02,  8.5416e-02, -1.2506e-01],\n",
            "          [ 3.6130e-02, -9.7982e-02,  3.2898e-02,  5.1004e-02],\n",
            "          [ 1.2367e-01,  8.5236e-02, -1.0227e-01, -9.3459e-02],\n",
            "          [ 4.0699e-02,  4.4356e-02,  8.4289e-02, -3.5194e-02]],\n",
            "\n",
            "         [[-8.3795e-05,  5.5583e-03, -4.5060e-02,  1.3093e-01],\n",
            "          [-1.0123e-01,  1.0352e-02, -1.1273e-01, -1.3254e-01],\n",
            "          [ 4.6079e-02,  1.1649e-01, -1.0002e-01, -7.6346e-02],\n",
            "          [-1.3259e-01,  1.3044e-01, -1.4191e-01,  4.4792e-02]],\n",
            "\n",
            "         [[ 1.3471e-01, -3.9488e-02, -6.4988e-02, -1.8768e-02],\n",
            "          [ 3.1070e-02,  6.1362e-02, -1.1865e-01,  1.8100e-02],\n",
            "          [ 9.1571e-02, -4.5619e-02, -5.9027e-02,  8.7101e-02],\n",
            "          [ 1.3368e-01, -4.1281e-02,  1.0171e-01, -5.4716e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 8.9364e-02,  9.2135e-02,  2.1648e-02,  1.0829e-01],\n",
            "          [-4.1639e-02,  3.3803e-02, -4.3917e-02, -1.8002e-02],\n",
            "          [ 1.0807e-02,  9.4352e-02,  4.1833e-02,  7.8956e-02],\n",
            "          [ 9.9197e-02,  9.6807e-02,  7.8831e-03,  9.4473e-02]],\n",
            "\n",
            "         [[ 9.0872e-02, -8.1991e-02, -4.5649e-02, -1.2585e-01],\n",
            "          [-1.0734e-01, -6.4334e-02,  2.6349e-02, -1.1556e-01],\n",
            "          [-1.0273e-01,  8.3178e-02,  8.1310e-02,  1.1887e-01],\n",
            "          [-2.7606e-02, -1.3665e-01, -1.3038e-01,  2.3440e-02]],\n",
            "\n",
            "         [[-3.4384e-02,  9.1761e-02, -1.0344e-01, -1.3789e-01],\n",
            "          [ 7.9820e-02,  7.8578e-02, -4.2243e-02,  1.1843e-01],\n",
            "          [ 1.1962e-01, -1.3322e-01, -2.2327e-02,  6.6085e-03],\n",
            "          [ 9.6209e-02,  9.2215e-02,  6.9578e-02,  9.1277e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 6.9056e-02, -1.8031e-02,  4.9997e-02,  6.4601e-02],\n",
            "          [ 5.7328e-02, -8.1557e-02, -1.0916e-01, -1.2481e-01],\n",
            "          [-4.2121e-02, -4.5059e-02,  1.4187e-01,  7.2134e-02],\n",
            "          [-4.5857e-02,  9.7485e-02,  7.3574e-02,  8.8535e-02]],\n",
            "\n",
            "         [[-8.9305e-02, -4.6043e-03, -1.0941e-01,  5.6807e-02],\n",
            "          [ 6.2637e-02, -1.1237e-01, -1.5618e-02,  8.1976e-02],\n",
            "          [-9.3800e-02, -9.1307e-02, -2.9402e-02,  5.1282e-02],\n",
            "          [-4.8426e-02, -3.3172e-02,  8.5331e-03, -9.0497e-02]],\n",
            "\n",
            "         [[-1.1386e-01, -2.0072e-02,  5.0709e-02,  6.2773e-02],\n",
            "          [ 1.5440e-02,  2.1511e-02, -7.9559e-02,  1.2858e-01],\n",
            "          [-1.2731e-01, -3.7875e-02,  3.9966e-02,  4.6685e-02],\n",
            "          [ 1.3808e-01, -1.2210e-01, -4.9833e-02, -6.6020e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3882e-01,  1.3282e-01,  1.1287e-01,  6.4830e-02],\n",
            "          [-1.0265e-02, -1.3920e-01,  6.9130e-04, -9.6104e-02],\n",
            "          [-9.7412e-02, -1.3286e-01,  1.3316e-01,  8.7602e-02],\n",
            "          [-1.0845e-02,  4.1280e-02, -1.3122e-01, -2.1338e-02]],\n",
            "\n",
            "         [[ 1.3600e-01, -2.2435e-02, -8.2690e-02, -1.1735e-01],\n",
            "          [ 8.4829e-02, -5.1090e-02, -1.3497e-01,  5.0020e-02],\n",
            "          [-5.5535e-02, -7.6921e-03, -3.3037e-03, -1.3552e-01],\n",
            "          [ 7.1731e-02, -2.5318e-02, -7.3505e-02,  1.4203e-01]],\n",
            "\n",
            "         [[-1.3202e-01, -1.2904e-01,  4.5657e-02, -1.0026e-01],\n",
            "          [-1.2059e-01, -4.5010e-02,  3.8271e-02, -6.8682e-02],\n",
            "          [-1.3581e-01, -5.4653e-02, -2.5948e-02, -1.3187e-01],\n",
            "          [-1.0839e-01,  1.3922e-02,  2.3742e-02, -9.7114e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.0469e-02, -1.3433e-01, -1.7134e-02, -3.0214e-02],\n",
            "          [ 7.2367e-02,  7.8201e-03,  7.6227e-02,  5.1794e-02],\n",
            "          [ 8.8285e-02, -8.0316e-02, -5.4052e-02,  1.2322e-01],\n",
            "          [ 9.2702e-03, -4.6334e-02,  1.4142e-01, -1.6733e-02]],\n",
            "\n",
            "         [[-2.5502e-02, -9.8460e-02, -1.4262e-02,  1.4137e-01],\n",
            "          [ 7.6073e-02, -4.5406e-02,  1.0293e-02, -7.3597e-03],\n",
            "          [-3.3226e-02,  3.5159e-02,  1.3730e-01,  6.9773e-02],\n",
            "          [-3.0747e-03, -1.0138e-01,  1.3936e-01,  1.1729e-01]],\n",
            "\n",
            "         [[ 1.1022e-01,  1.3190e-01, -1.0338e-01, -1.4171e-01],\n",
            "          [ 7.8381e-02,  1.1363e-01, -8.0010e-02, -5.6346e-02],\n",
            "          [ 3.9353e-02, -8.9082e-02,  8.1554e-02, -9.4829e-02],\n",
            "          [ 3.6571e-02,  8.4917e-02, -5.1280e-02,  9.2920e-04]]],\n",
            "\n",
            "\n",
            "        [[[-7.7965e-02, -7.5794e-02, -3.8872e-02,  1.0786e-01],\n",
            "          [-9.9210e-03,  4.7429e-02,  1.5106e-02, -5.1320e-02],\n",
            "          [-7.8897e-02, -6.8420e-02,  9.9780e-02,  9.0345e-02],\n",
            "          [ 3.8743e-02,  9.8414e-02,  1.2293e-01,  1.0319e-01]],\n",
            "\n",
            "         [[ 2.3467e-02, -1.2598e-01,  8.6410e-02,  8.4582e-03],\n",
            "          [-8.6442e-02,  1.3564e-02, -7.7918e-02, -1.2058e-01],\n",
            "          [-8.0539e-02,  1.1537e-01, -1.1111e-01, -7.3311e-02],\n",
            "          [-1.2885e-02, -2.7330e-02, -1.4167e-01, -5.9118e-02]],\n",
            "\n",
            "         [[ 6.6838e-02, -5.2045e-02, -8.6197e-02, -5.7354e-02],\n",
            "          [-2.1154e-02,  4.3257e-02, -1.2109e-01,  3.4212e-02],\n",
            "          [-9.9848e-03, -5.3252e-02, -1.0728e-01,  9.9283e-02],\n",
            "          [-3.2915e-02,  1.0350e-01, -8.5834e-02, -8.2287e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.5236e-02, -7.5834e-02, -2.3905e-02, -4.1215e-02],\n",
            "          [ 6.5764e-02, -1.5623e-03,  2.1015e-02, -5.1007e-02],\n",
            "          [-1.8462e-02, -1.1671e-01, -1.1405e-01, -1.4146e-01],\n",
            "          [-1.4359e-01, -1.1416e-01, -3.5316e-02, -2.0736e-02]],\n",
            "\n",
            "         [[-1.1256e-01,  5.8160e-02,  1.3992e-01,  4.2234e-02],\n",
            "          [ 5.8342e-02,  1.3174e-01,  8.8899e-02, -5.7670e-02],\n",
            "          [-7.0654e-02, -2.8420e-02, -5.9238e-02,  6.8089e-02],\n",
            "          [-8.1554e-02, -3.4947e-03,  9.6300e-02,  1.2267e-01]],\n",
            "\n",
            "         [[ 2.6802e-03, -2.1430e-02, -1.0989e-01,  9.2970e-02],\n",
            "          [-7.9999e-02, -1.0523e-01, -1.3017e-01,  8.7649e-02],\n",
            "          [-1.3176e-01,  7.1732e-02,  1.6747e-02,  4.4697e-02],\n",
            "          [-8.3436e-02, -1.3986e-01,  3.4817e-03,  8.3124e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.2877e-02, -1.3004e-01,  8.0916e-02,  8.4763e-02],\n",
            "          [-5.4003e-03, -4.2902e-02,  7.2628e-03,  9.0924e-02],\n",
            "          [ 3.5054e-03, -2.0093e-02, -5.5302e-02, -7.6611e-02],\n",
            "          [ 3.5658e-02, -1.1636e-01,  1.1175e-01,  7.3954e-02]],\n",
            "\n",
            "         [[-1.0863e-01,  6.6036e-02, -9.4528e-02, -9.2235e-02],\n",
            "          [-1.0635e-01,  1.1121e-01,  1.1447e-01, -1.2683e-01],\n",
            "          [-1.6672e-02,  2.5151e-02,  1.1097e-01,  5.2307e-02],\n",
            "          [-8.5864e-03, -4.0675e-02, -1.0411e-01, -3.3714e-02]],\n",
            "\n",
            "         [[-8.7812e-02,  6.2474e-03,  8.0430e-02, -1.6344e-02],\n",
            "          [-6.9054e-02, -2.2581e-02, -6.7219e-02,  1.3281e-01],\n",
            "          [ 7.6791e-02, -1.0336e-01,  3.4250e-02,  1.4146e-01],\n",
            "          [-9.3799e-02,  1.2835e-01,  1.3815e-01,  1.0758e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.4277e-01,  5.8758e-02,  9.9624e-02, -6.7041e-02],\n",
            "          [ 3.1573e-02,  5.6812e-02, -1.0095e-01, -1.0976e-02],\n",
            "          [-1.1075e-01,  7.7981e-02, -5.5281e-02,  1.9649e-02],\n",
            "          [ 1.2143e-01,  1.0707e-01, -9.8875e-02,  7.2252e-02]],\n",
            "\n",
            "         [[-1.4377e-01,  6.8719e-02, -6.3339e-02, -1.2374e-01],\n",
            "          [-1.0895e-01,  1.1960e-01, -1.1689e-01,  7.2250e-02],\n",
            "          [-1.0341e-01,  1.9429e-02,  5.5845e-02,  1.1277e-01],\n",
            "          [-1.1857e-02, -3.1732e-02,  4.4236e-03, -5.4908e-02]],\n",
            "\n",
            "         [[-9.6935e-02,  5.5392e-02,  1.4248e-01,  1.0986e-01],\n",
            "          [-5.1107e-02,  1.2635e-01, -1.3266e-01,  2.0267e-02],\n",
            "          [-3.4608e-02, -8.6926e-02, -4.7908e-02,  1.3412e-01],\n",
            "          [-2.5965e-02,  8.7372e-02,  9.0105e-02,  1.1700e-01]]]],\n",
            "       requires_grad=True)\n",
            "Bias of conv_layer:  Parameter containing:\n",
            "tensor([ 0.0270, -0.1244,  0.0425,  0.0443,  0.0080,  0.1058,  0.0463,  0.1253,\n",
            "        -0.1332,  0.1303, -0.0151,  0.0585,  0.0220, -0.0368,  0.1217,  0.1118],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some gyan about 2D Pooling Layer\n",
        "\n",
        "# Pooling layers are used to downsample the input, reducing its spatial dimensions while\n",
        "# retaining important information. This helps make the model more computationally efficient\n",
        "# and less prone to overfitting. There are two main types of pooling layers in\n",
        "# PyTorch: MaxPool2d (max pooling) and AvgPool2d (average pooling).\n",
        "# The key arguments of a pooling layer are kernel_size, stride, and padding:\n",
        "# - kernel_size specifies the size of the window (typically (height, width)) over which pooling is applied.\n",
        "# - stride defines how much the pooling window moves across the input. If not specified, it defaults to the value of kernel_size.\n",
        "# - padding adds zero-padding around the input before pooling (typically not used in pooling layers, but it can be).\n",
        "# Example of Max Pooling:\n",
        "# maxpool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "# In this case, the MaxPool2d layer applies max pooling over a 2x2 window, moving with a stride of 2,\n",
        "# which effectively reduces the spatial dimensions of the input by half. If the input shape is\n",
        "# (batch_size, channels, height, width), the output shape after pooling will be (batch_size, channels, height//2, width//2).\n",
        "# Similarly, we can use Average Pooling (AvgPool2d) to compute the average value in each pooling window:\n",
        "# avgpool_layer = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "# To apply pooling to an input tensor:\n",
        "# pooled_output = maxpool_layer(input)\n",
        "\n",
        "# As with convolutional layers, pooling layers have no trainable parameters like weights or biases.\n",
        "# Their role is purely to downsample the input, so there’s no need to call .parameters().\n",
        "# A common practice is to apply pooling after convolutional layers to reduce the spatial dimensions\n",
        "# while preserving the depth (number of channels). Pooling helps make the network more computationally\n",
        "# efficient while maintaining the most important features.\n",
        "# Here’s an example of applying both convolution and max pooling together:\n",
        "# conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "# maxpool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "# After a convolutional operation:\n",
        "# conv_output = conv_layer(input) # the dimension of output layer (batch_size, in_channels, new_height, new_width)\n",
        "\n",
        "# Apply max pooling to reduce spatial dimensions:\n",
        "# pooled_output = maxpool_layer(conv_output)\n",
        "\n",
        "# Note that pooling is typically used after an activation function (like ReLU) to preserve non-linearity:\n",
        "# relu_output = nn.ReLU()(conv_output) pooled_output = maxpool_layer(relu_output)\n",
        "\n",
        "# Like in the convolution layer, if you want to inspect the effects of pooling on an input, you can print\n",
        "# the shape of the input and output tensors before and after pooling:\n",
        "# print(\"Shape before pooling: \", conv_output.shape) print(\"Shape after pooling: \", pooled_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgfZj2_CWbCj",
        "outputId": "de4e2a4c-9f1d-4b5c-c407-457f1c42bddb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some gyan about transitioning from Conv2D to a Fully Connected Layer\n",
        "\n",
        "# In Convolutional Neural Networks (CNNs), after several convolutional\n",
        "# and pooling layers, the output tensor still retains a 3D structure\n",
        "#  (i.e., with dimensions corresponding to height, width, and depth/channels).\n",
        "# the order being (channels, height, width)\n",
        "#  However, to apply a fully connected layer (or nn.Linear), this 3D output\n",
        "#  needs to be flattened into a 1D vector. This allows the fully connected\n",
        "#  layer to treat the flattened tensor as a simple vector of inputs, just\n",
        "#  like in traditional feedforward networks.\n",
        "# To understand the transition, let's break it down:\n",
        "# Output of Conv/Pooling Layer: After a series of convolutional and pooling\n",
        "# layers, the output typically has the shape (batch_size, channels, height, width).\n",
        "# For example, say we have a tensor of shape (batch_size=32, channels=16, height=8, width=8).\n",
        "\n",
        "# Flattening the Output: To connect this 3D tensor to a fully connected layer, you\n",
        "# first need to \"flatten\" it, meaning you collapse the (channels, height, width)\n",
        "# dimensions into a single 1D vector. This can be done using torch.flatten() or\n",
        "# reshaping the tensor manually using .view() or .reshape() methods.\n",
        "\n",
        "# --code:\n",
        "\n",
        "conv_output = torch.randn(32, 16, 8, 8)  # Example output of a conv layer\n",
        "flattened_output = conv_output.view(32, -1)  # Flattening the tensor (32, 16*8*8)\n",
        "# The flattened tensor will now have shape (batch_size, 16*8*8). In this case, (32, 1024).\n",
        "\n",
        "# Fully Connected Layer After Flattening: Now that we have flattened the output, we can pass\n",
        "# it to a fully connected (nn.Linear) layer. The number of input features (in_features) for\n",
        "# this layer should match the size of the flattened vector (i.e., 16 * 8 * 8 = 1024 in this example).\n",
        "#  The number of output features (out_features) is the number of neurons you want in the next layer,\n",
        "#  depending on your task (e.g., 10 for a 10-class classification problem).\n",
        "\n",
        "\n",
        "fc_layer = nn.Linear(in_features=16*8*8, out_features=10)  # Example fully connected layer\n",
        "\n",
        "# Science Behind the Transition:\n",
        "\n",
        "# Why Flatten? The convolutional layers capture spatial features (edges, textures, shapes)\n",
        "# from the input, and the output tensor retains this spatial structure. However, fully\n",
        "#  connected layers expect a simple vector of features. Flattening helps \"unroll\" the\n",
        "#  learned spatial features into a 1D vector so that the fully connected layer can process\n",
        "#  them for the final decision-making part of the network (e.g., classification).\n",
        "\n",
        "# Why Fully Connected Layer? Once the convolutional layers have distilled the spatial\n",
        "# information into meaningful high-level features, the fully connected layers serve as a\n",
        "# \"classifier\" or \"decision-maker\" that takes these abstracted features and assigns\n",
        "# them to the correct class, or performs regression if it's a different task.\n",
        "\n",
        "# Putting It All Together: Here's an example of how you'd go from a convolutional layer to a fully connected layer in a simple CNN:\n",
        "\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # After conv and pooling, the output size is (batch_size, 16, 8, 8)\n",
        "        self.fc_layer = nn.Linear(in_features=16*8*8, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)  # Conv layer\n",
        "        x = self.pool(x)        # Pooling layer\n",
        "        x = x.view(x.size(0), -1)  # Flattening the output (batch_size, 1024)\n",
        "        x = self.fc_layer(x)    # Fully connected layer\n",
        "        return x\n",
        "\n",
        "# In this example, the conv_layer processes the input, the pool downscales it,\n",
        "# and then the output is flattened to feed into the fully connected layer. This\n",
        "#  transition allows us to combine feature extraction (via convolutions) with\n",
        "#  classification (via fully connected layers)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpoladeXM-qm",
        "outputId": "4edfb05d-a0c3-4515-9caa-952e21422343"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LEARNING ABOUT RETRIEVING INDIVIDUAL LAYERS AND MAKING REQUIRED CHANGES"
      ],
      "metadata": {
        "id": "9qen3nfnhsHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some gyan on retrieving individual layers of a pytorch models\n",
        "\n",
        "# Let's first start with a convolution model built by subclassing nn.Module\n",
        "\n",
        "class MyCustomModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 4)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 4)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 4)\n",
        "    self.pool = nn.MaxPool2d(kernel_size = 4, stride = 4)\n",
        "    self.fc1 = nn.Linear(in_features = 128, out_features = 56)\n",
        "    self.fc2 = nn.Linear(in_features = 56, out_features = 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.pool(x)\n",
        "    x = torch.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.pool(x)\n",
        "    x = torch.relu(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.pool(x)\n",
        "    x = torch.relu(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc1(x)\n",
        "    x = torch.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "custom_model = MyCustomModel()\n",
        "\n",
        "# we can checkout all the layers of this model as follows:\n",
        "for layer in custom_model.named_children():\n",
        "  print(layer)\n",
        "\n",
        "# the reposponse is:\n",
        "# ('conv1', Conv2d(3, 8, kernel_size=(4, 4), stride=(1, 1)))\n",
        "# ('conv2', Conv2d(8, 16, kernel_size=(4, 4), stride=(1, 1)))\n",
        "# ('conv3', Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1)))\n",
        "# ('pool', MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False))\n",
        "# ('fc1', Linear(in_features=128, out_features=56, bias=True))\n",
        "# ('fc2', Linear(in_features=56, out_features=10, bias=True))\n",
        "\n",
        "# From the response above, I am able to see all the layers, however I can not understand\n",
        "# the flow from input to output, as this pooling layer is used many times which is unclear\n",
        "# from here. Note: It actually picks just layers only, like if i try to confuse it by\n",
        "# initialiting self.my_layer = \"just to confuse\", it will not call it as a layer as expected.\n",
        "# Next we will learn picking up a layer from here and manipulating it:\n",
        "\n",
        "# From printing the custom_model.named_childeren(), we know that the conv layers with names conv1,\n",
        "# conv2, conv3 are there, lets say I want to manipulate the weights of it:\n",
        "\n",
        "with torch.no_grad():\n",
        "  custom_model.conv1.weight *= 2\n",
        "  custom_model.conv2.bias = nn.Parameter(torch.zeros(custom_model.conv2.bias.shape))\n",
        "  custom_model.fc1.weight = nn.Parameter(torch.randn(custom_model.fc1.weight.shape))\n",
        "\n",
        "for param in custom_model.conv2.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuRhqpAvPfQe",
        "outputId": "3b618851-4416-4b87-ef77-8a449dcb419f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('conv1', Conv2d(3, 8, kernel_size=(4, 4), stride=(1, 1)))\n",
            "('conv2', Conv2d(8, 16, kernel_size=(4, 4), stride=(1, 1)))\n",
            "('conv3', Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1)))\n",
            "('pool', MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False))\n",
            "('fc1', Linear(in_features=128, out_features=56, bias=True))\n",
            "('fc2', Linear(in_features=56, out_features=10, bias=True))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we would learn how to checkout the flow of the model, i.e from input to output what path it goes through\n",
        "\n",
        "# One simple way is to print the shape and layer name within the forward function while definig the model.\n",
        "# That is too simple to discuss, lets look at a better and flxible way of doing this, using hook. Let's see how\n",
        "# that can be done\n",
        "\n",
        "def print_hook(module, input, output):\n",
        "  print(f\"{module}: \\nOutput shape: {output.shape}\\n\")\n",
        "\n",
        "# Register hooks on layers\n",
        "for layer in custom_model.named_children():\n",
        "  layer[1].register_forward_hook(print_hook)\n",
        "\n",
        "# Now, we can pass the data though the model to trigger the hooks\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "output = custom_model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbZlqP2VfpP5",
        "outputId": "b60d173f-0d05-423e-a067-f733ec2ba35e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d(3, 8, kernel_size=(4, 4), stride=(1, 1)): \n",
            "Output shape: torch.Size([1, 8, 221, 221])\n",
            "\n",
            "MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False): \n",
            "Output shape: torch.Size([1, 8, 55, 55])\n",
            "\n",
            "Conv2d(8, 16, kernel_size=(4, 4), stride=(1, 1)): \n",
            "Output shape: torch.Size([1, 16, 52, 52])\n",
            "\n",
            "MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False): \n",
            "Output shape: torch.Size([1, 16, 13, 13])\n",
            "\n",
            "Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1)): \n",
            "Output shape: torch.Size([1, 32, 10, 10])\n",
            "\n",
            "MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False): \n",
            "Output shape: torch.Size([1, 32, 2, 2])\n",
            "\n",
            "Linear(in_features=128, out_features=56, bias=True): \n",
            "Output shape: torch.Size([1, 56])\n",
            "\n",
            "Linear(in_features=56, out_features=10, bias=True): \n",
            "Output shape: torch.Size([1, 10])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing the above steps for pretranied models\n",
        "\n",
        "# Load a pre-trained ResNet model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Print the model architecture\n",
        "# print(model)\n",
        "\n",
        "\n",
        "# for name, layer in model.named_children():\n",
        "#   print(name)\n",
        "#   print(layer)\n",
        "#   print(\" \")\n",
        "#   print(\" \")\n",
        "\n",
        "# now in order to make changes to any layer, like conv1\n",
        "# with torch.no_grad():\n",
        "#   model.conv1.weight *= 2.0\n",
        "#   model.fc.weight = nn.Parameter(torch.randn(model.fc.weight.shape))\n",
        "\n",
        "# # Also, getting to know the flow is also straightforward\n",
        "def print_hook(module, input, output):\n",
        "  print(f\"{module}: \\nOutput shape: {output.shape}\\n\")\n",
        "\n",
        "# # Register hooks on layers\n",
        "for layer in model.named_children():\n",
        "  layer[1].register_forward_hook(print_hook)\n",
        "\n",
        "# Now, we can pass the data though the model to trigger the hooks\n",
        "x = torch.randn(10, 3, 224, 224)\n",
        "output = model(x)\n",
        "\n",
        "\n",
        "# Also we see here, there are many blocks which are made of nn.Sequential, we discuss about it in the next cell."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LfOpMQ1iNBh",
        "outputId": "46f7d667-4c68-4a20-ddc7-6fe0bf904bfe"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False): \n",
            "Output shape: torch.Size([10, 64, 112, 112])\n",
            "\n",
            "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True): \n",
            "Output shape: torch.Size([10, 64, 112, 112])\n",
            "\n",
            "ReLU(inplace=True): \n",
            "Output shape: torch.Size([10, 64, 112, 112])\n",
            "\n",
            "MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False): \n",
            "Output shape: torch.Size([10, 64, 56, 56])\n",
            "\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "): \n",
            "Output shape: torch.Size([10, 64, 56, 56])\n",
            "\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "): \n",
            "Output shape: torch.Size([10, 128, 28, 28])\n",
            "\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "): \n",
            "Output shape: torch.Size([10, 256, 14, 14])\n",
            "\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "): \n",
            "Output shape: torch.Size([10, 512, 7, 7])\n",
            "\n",
            "AdaptiveAvgPool2d(output_size=(1, 1)): \n",
            "Output shape: torch.Size([10, 512, 1, 1])\n",
            "\n",
            "Linear(in_features=512, out_features=1000, bias=True): \n",
            "Output shape: torch.Size([10, 1000])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-BhMp9mShzTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LEARNING NN.SEQUENTIAL"
      ],
      "metadata": {
        "id": "DTFRbyYxh0pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Sequential in PyTorch is a convenient way to define models where the layers are stacked sequentially,\n",
        "# one after the other. It simplifies the process of creating a model, especially when the layers are executed\n",
        "# in a straightforward manner, meaning each layer’s output is passed as the input to the next layer.\n",
        "\n",
        "# Basic Idea\n",
        "# Instead of subclassing nn.Module and defining the forward method manually, you can stack layers using\n",
        "# nn.Sequential, which automatically assumes that the input will be passed through the layers in the\n",
        "# order they are defined.\n",
        "\n",
        "# Key Features of nn.Sequential:\n",
        "# Simplified Layer Stacking: You can define a model by simply listing the layers in the order they are applied.\n",
        "# Automatic Forward Pass: The forward pass is implicitly defined as the input passes through all layers sequentially.\n",
        "# Layer Manipulation: You can easily access and manipulate individual layers.\n",
        "# Example: Basic Usage\n",
        "# Let’s say you want to define a neural network with:\n",
        "\n",
        "# A fully connected (dense) layer (nn.Linear),\n",
        "# A ReLU activation function (nn.ReLU),\n",
        "# Another fully connected layer.\n",
        "# You can create this simple model using nn.Sequential:\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a model using nn.Sequential\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(4, 10),  # Input features = 4, Output features = 10\n",
        "    nn.ReLU(),         # Activation function\n",
        "    nn.Linear(10, 2)   # Input features = 10, Output features = 2\n",
        ")\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)\n",
        "\n",
        "# Sequential(\n",
        "#   (0): Linear(in_features=4, out_features=10, bias=True)\n",
        "#   (1): ReLU()\n",
        "#   (2): Linear(in_features=10, out_features=2, bias=True)\n",
        "# )\n",
        "\n",
        "\n",
        "# Forward Pass\n",
        "# Now, if you input a tensor to the model, it will automatically go through the layers in order:\n",
        "\n",
        "# Create a random input tensor of shape (batch_size, input_features)\n",
        "input_tensor = torch.randn(3, 4)  # Batch size = 3, Input features = 4\n",
        "\n",
        "# Forward pass through the model\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(output)\n",
        "\n",
        "# tensor([[-0.0314,  0.2432],\n",
        "#         [-0.1920,  0.2143],\n",
        "#         [-0.0284,  0.0652]], grad_fn=<AddmmBackward0>)\n",
        "\n",
        "\n",
        "# Custom Layer Names\n",
        "# If you want to give specific names to each layer, you can use an OrderedDict to specify both the names and the layers:\n",
        "\n",
        "\n",
        "# from collections import OrderedDict\n",
        "\n",
        "# # Define a model using nn.Sequential with named layers\n",
        "# model = nn.Sequential(OrderedDict([\n",
        "#     ('fc1', nn.Linear(4, 10)),  # First fully connected layer\n",
        "#     ('relu', nn.ReLU()),        # ReLU activation\n",
        "#     ('fc2', nn.Linear(10, 2))   # Second fully connected layer\n",
        "# ]))\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)\n",
        "# Accessing Layers in nn.Sequential\n",
        "# You can access individual layers in nn.Sequential using indices, just like accessing elements in a list:\n",
        "\n",
        "# Access the first layer (fc1)\n",
        "print(model[0])  # This will print nn.Linear(4, 10)\n",
        "\n",
        "# Access the activation function (ReLU)\n",
        "print(model[1])  # This will print nn.ReLU()\n",
        "\n",
        "# Modify or replace a layer\n",
        "model[0] = nn.Linear(4, 20)  # Change the first layer to have 20 output features\n",
        "\n",
        "# Print updated model\n",
        "print(model)"
      ],
      "metadata": {
        "id": "KTRYuygIeOI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1DTbSoxrh5ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LEARNING LEARNING RATE SCHEDULER"
      ],
      "metadata": {
        "id": "d2ww6IXZh6R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we learn how to schedule learning rate of the model in pytorch\n",
        "\n",
        "# A learning rate scheduler makes changes to the learning rate during the training process,\n",
        "# makes the training attain different leraning rates as per the plan\n",
        "\n",
        "# Lets dicuss three types of scheduler, will provide the entrire custom loop for two of them\n",
        "\n",
        "# StepLR: Reduces the learning rate by a factor every few epochs.\n",
        "# ExponentialLR: Decays the learning rate exponentially.\n",
        "# ReduceLROnPlateau: Reduces the learning rate when a metric stops improving.\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define a simple model for demonstration\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Dummy dataset and dataloader\n",
        "train_data = torch.randn(100, 10)\n",
        "train_labels = torch.randint(0, 2, (100,))\n",
        "train_loader = DataLoader(list(zip(train_data, train_labels)), batch_size=10)\n",
        "\n",
        "# Define loss function and model\n",
        "model = MyModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Define StepLR scheduler: reduce lr by 0.1 every 10 epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) # so, after every 10 epochs the new learnng rate is gamma*existing_learning_rate\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero gradients\n",
        "\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Step the scheduler at the end of each epoch\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Learning Rate: {scheduler.get_last_lr()[0]}')\n",
        "\n",
        "# Everythings in training loop remain same, just that after every epoch we just run scheduler.step()\n",
        "# which upodates the info in the scheudler and scheduler make necssary changes when requried, i.e for\n",
        "#  stepLR it will make the learning_rate as 0.1*learnig_rate after 10 epochs.\n",
        "\n",
        "\n",
        "# Lets now jump into the ReduceLROnPlateau, its discussed in next section"
      ],
      "metadata": {
        "id": "aqueHU0C-PEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The ReduceLROnPlateau learning rate scheduler is designed to adjust the learning rate dynamically\n",
        "# when a specific metric (often validation loss) stops improving. Instead of reducing the learning\n",
        "# rate after a fixed number of epochs, like in StepLR, this scheduler reduces the learning rate when\n",
        "# the monitored metric plateaus—i.e., when the metric (like validation loss) does not improve for a\n",
        "# certain number of epochs. This allows the model to continue learning effectively when improvement\n",
        "# slows down, without wasting time on a high learning rate that might already be too aggressive.\n",
        "\n",
        "# How ReduceLROnPlateau Works:\n",
        "# Monitored Metric: Typically, the validation loss is monitored, though it can be any other metric.\n",
        "# Patience: The scheduler waits for a defined number of epochs (patience) without improvement in the monitored metric before reducing the learning rate.\n",
        "# Factor: The learning rate is reduced by a certain factor (e.g., by multiplying it by 0.1 or another value).\n",
        "# Mode: The scheduler can operate in either min mode (to reduce the learning rate when the metric stops decreasing, which is typical for loss) or max mode (to reduce when the metric stops increasing, for metrics like accuracy).\n",
        "# Parameters:\n",
        "# mode='min': The scheduler reduces the learning rate when the validation loss does not decrease.\n",
        "# factor=0.1: When triggered, the learning rate is multiplied by this factor.\n",
        "# patience=5: The scheduler waits for 5 epochs of no improvement before reducing the learning rate.\n",
        "# verbose: Whether or not to print a message when the learning rate is reduced.\n",
        "# threshold: The amount of improvement considered significant.\n",
        "# cooldown: The number of epochs to wait after the learning rate has been reduced before monitoring again.\n",
        "\n",
        "# In the Training Loop:\n",
        "# In your training loop, ReduceLROnPlateau is used to adjust the learning rate based on the validation loss. Here's a breakdown of how it operates within your code:\n",
        "\n",
        "# Key Steps:\n",
        "# Training Phase:\n",
        "\n",
        "# Each epoch, the model is trained on the training data, and the training loss is accumulated.\n",
        "# The optimizer updates the model's weights using backpropagation and the learning rate determined by the current state of the optimizer.\n",
        "\n",
        "# Validation Phase:\n",
        "\n",
        "# After training, the model is evaluated on the validation dataset to compute the validation loss (val_loss).\n",
        "# This loss is the key metric being monitored to determine if the learning rate should be reduced.\n",
        "# Scheduler Step:\n",
        "\n",
        "# At the end of each epoch, the scheduler is stepped using the validation loss: scheduler.step(val_loss).\n",
        "# If the validation loss doesn't improve for patience consecutive epochs, the learning rate is reduced by\n",
        "# multiplying it by factor. For example, if the current learning rate is 0.1 and factor is 0.1, the\n",
        "# learning rate will be reduced to 0.01. The learning rate is updated dynamically as training progresses.\n",
        "\n",
        "# Why Use ReduceLROnPlateau:\n",
        "# In deep learning, models often hit points where the learning stagnates (loss doesn't decrease or decreases\n",
        "# very slowly). A high learning rate might prevent the model from fine-tuning, while a lower learning rate\n",
        "# can help it converge to a better minimum. ReduceLROnPlateau helps manage this by reducing the learning\n",
        "# rate only when it's needed, making the training process more efficient.\n",
        "\n",
        "# Now the training loop:\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Same model as before\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Dummy datasets and dataloaders\n",
        "train_data = torch.randn(100, 10)\n",
        "train_labels = torch.randint(0, 2, (100,))\n",
        "train_loader = DataLoader(list(zip(train_data, train_labels)), batch_size=10)\n",
        "\n",
        "val_data = torch.randn(40, 10)\n",
        "val_labels = torch.randint(0, 2, (40,))\n",
        "val_loader = DataLoader(list(zip(val_data, val_labels)), batch_size=10)\n",
        "\n",
        "# Define loss function and model\n",
        "model = MyModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Define ReduceLROnPlateau scheduler: reduce lr by 0.1 if no improvement in 5 epochs\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "# Function to evaluate on validation set\n",
        "def validate(model, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    return val_loss / len(val_loader)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero gradients\n",
        "\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss = validate(model, val_loader)\n",
        "\n",
        "    # Step the scheduler based on validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}, Learning Rate: {optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "\n",
        "# We can also create the custom learning rate scheduler, it can be done by subclassing _LRScheduler, we would need to\n",
        "# look at the structure of this class to implement that, but it could be easily found online.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us1YHzbZkF1W",
        "outputId": "0d6d3d29-d3f2-4bf8-efa6-aaef9189059c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 0.7788966029882431, Val Loss: 0.7355731874704361, Learning Rate: 0.1\n",
            "Epoch 2/30, Loss: 0.6917988240718842, Val Loss: 0.7289204001426697, Learning Rate: 0.1\n",
            "Epoch 3/30, Loss: 0.6631533682346344, Val Loss: 0.7389393299818039, Learning Rate: 0.1\n",
            "Epoch 4/30, Loss: 0.6543763160705567, Val Loss: 0.7502733469009399, Learning Rate: 0.1\n",
            "Epoch 5/30, Loss: 0.6516462564468384, Val Loss: 0.7594905197620392, Learning Rate: 0.1\n",
            "Epoch 6/30, Loss: 0.6508108675479889, Val Loss: 0.7663657814264297, Learning Rate: 0.1\n",
            "Epoch 7/30, Loss: 0.6505915224552155, Val Loss: 0.7713499069213867, Learning Rate: 0.1\n",
            "Epoch 8/30, Loss: 0.6505707085132599, Val Loss: 0.7749321162700653, Learning Rate: 0.010000000000000002\n",
            "Epoch 9/30, Loss: 0.6329628229141235, Val Loss: 0.7749206572771072, Learning Rate: 0.010000000000000002\n",
            "Epoch 10/30, Loss: 0.6327947974205017, Val Loss: 0.7749110907316208, Learning Rate: 0.010000000000000002\n",
            "Epoch 11/30, Loss: 0.6326460301876068, Val Loss: 0.7749034911394119, Learning Rate: 0.010000000000000002\n",
            "Epoch 12/30, Loss: 0.6325141608715057, Val Loss: 0.7748972028493881, Learning Rate: 0.010000000000000002\n",
            "Epoch 13/30, Loss: 0.6323971927165986, Val Loss: 0.7748922109603882, Learning Rate: 0.010000000000000002\n",
            "Epoch 14/30, Loss: 0.6322933852672576, Val Loss: 0.774888426065445, Learning Rate: 0.0010000000000000002\n",
            "Epoch 15/30, Loss: 0.6303412795066834, Val Loss: 0.7748857587575912, Learning Rate: 0.0010000000000000002\n",
            "Epoch 16/30, Loss: 0.6303310990333557, Val Loss: 0.7748830765485764, Learning Rate: 0.0010000000000000002\n",
            "Epoch 17/30, Loss: 0.6303210318088531, Val Loss: 0.7748803794384003, Learning Rate: 0.0010000000000000002\n",
            "Epoch 18/30, Loss: 0.6303110957145691, Val Loss: 0.7748777270317078, Learning Rate: 0.0010000000000000002\n",
            "Epoch 19/30, Loss: 0.6303013265132904, Val Loss: 0.7748751193284988, Learning Rate: 0.0010000000000000002\n",
            "Epoch 20/30, Loss: 0.6302916288375855, Val Loss: 0.7748724222183228, Learning Rate: 0.00010000000000000003\n",
            "Epoch 21/30, Loss: 0.6300953269004822, Val Loss: 0.7748721539974213, Learning Rate: 0.00010000000000000003\n",
            "Epoch 22/30, Loss: 0.6300943672657013, Val Loss: 0.7748718857765198, Learning Rate: 0.00010000000000000003\n",
            "Epoch 23/30, Loss: 0.6300934195518494, Val Loss: 0.7748716175556183, Learning Rate: 0.00010000000000000003\n",
            "Epoch 24/30, Loss: 0.630092453956604, Val Loss: 0.7748713791370392, Learning Rate: 0.00010000000000000003\n",
            "Epoch 25/30, Loss: 0.6300914824008942, Val Loss: 0.7748710513114929, Learning Rate: 0.00010000000000000003\n",
            "Epoch 26/30, Loss: 0.6300905168056488, Val Loss: 0.7748707234859467, Learning Rate: 1.0000000000000004e-05\n",
            "Epoch 27/30, Loss: 0.6300708711147308, Val Loss: 0.7748707234859467, Learning Rate: 1.0000000000000004e-05\n",
            "Epoch 28/30, Loss: 0.6300707995891571, Val Loss: 0.7748707383871078, Learning Rate: 1.0000000000000004e-05\n",
            "Epoch 29/30, Loss: 0.63007071018219, Val Loss: 0.7748707085847855, Learning Rate: 1.0000000000000004e-05\n",
            "Epoch 30/30, Loss: 0.6300706028938293, Val Loss: 0.7748706787824631, Learning Rate: 1.0000000000000004e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j3DKTzwFoEoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4M2Ci4_-kFk7"
      }
    }
  ]
}